{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4aaa43d-21ca-457a-affd-82b2d289f818",
   "metadata": {},
   "source": [
    "# Try Tiny BNN with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8ad8c1-45ae-4429-861a-cd06e97010f8",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f1b71b-6b55-44a6-b75a-83cc13602bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e32827cb-5c2d-4750-82b3-75c0e160c3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Using downloaded and verified file: ./data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Using downloaded and verified file: ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Using downloaded and verified file: ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Using downloaded and verified file: ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_data_train = torchvision.datasets.MNIST('./data/', \n",
    "                                        train=True, download=True,\n",
    "                                        transform=torchvision.transforms.Compose\n",
    "                                        ([\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "                                        ]))\n",
    "\n",
    "# Split the dataset into training and validation subsets\n",
    "train_size = int(0.8 * len(full_data_train))\n",
    "val_size = len(full_data_train) - train_size\n",
    "data_train, data_valid = random_split(full_data_train, [train_size, val_size])\n",
    "\n",
    "data_test = torchvision.datasets.MNIST('./data/', \n",
    "                                       train=False, download=True,\n",
    "                                       transform=torchvision.transforms.Compose\n",
    "                                       ([\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "                                       ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b41f69-98bd-4715-bb09-323e6fb749d3",
   "metadata": {},
   "source": [
    "## 2. Define MLP structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19753678-871a-469c-b544-c79048d99462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e1d3ae1-af90-4a5d-8d1c-f54a448fb719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brevitas.nn as qnn\n",
    "from brevitas.nn import QuantLinear, QuantReLU, QuantConv2d\n",
    "from brevitas.quant.binary import SignedBinaryActPerTensorConst\n",
    "from brevitas.quant.binary import SignedBinaryWeightPerTensorConst\n",
    "from brevitas.inject.enum import QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fffae836-5a45-4667-a622-b71b7b79eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1*28*28      \n",
    "hidden1 = 256      \n",
    "hidden2 = 256\n",
    "hidden3 = 256\n",
    "num_classes = 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e99d4da-52ea-4f61-88a3-6d1915f948c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BiKA_MNIST(Module):\n",
    "    def __init__(self):\n",
    "        super(BiKA_MNIST, self).__init__()\n",
    "        \n",
    "        self.input = qnn.QuantIdentity(quant_type='int', scaling_impl_type='const', bit_width=8, min_val=-128.0, max_val=127.0, return_quant_tensor=True)\n",
    "        \n",
    "        self.fc0   = qnn.QuantLinear(input_size, hidden1, weight_bit_width=8, weight_quant_type=QuantType.INT, bias=False)\n",
    "        self.bn0   = nn.BatchNorm1d(hidden1)\n",
    "        self.relu0 = qnn.QuantReLU(bit_width=8, return_quant_tensor=True)\n",
    "        \n",
    "        self.fc1   = qnn.QuantLinear(hidden1, hidden2, weight_bit_width=8, weight_quant_type=QuantType.INT, bias=False)\n",
    "        self.bn1   = nn.BatchNorm1d(hidden2)\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=8, return_quant_tensor=True)\n",
    "        \n",
    "        self.fc2   = qnn.QuantLinear(hidden2, hidden3, weight_bit_width=8, weight_quant_type=QuantType.INT, bias=False)\n",
    "        self.bn2   = nn.BatchNorm1d(hidden3)\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=8, return_quant_tensor=True)\n",
    "        \n",
    "        self.out   = qnn.QuantLinear(hidden3, num_classes, weight_bit_width=8, weight_quant_type=QuantType.INT, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = x.reshape(x.shape[0], -1)\n",
    "        out = self.input(out)\n",
    "        out = self.relu0(self.bn0(self.fc0(out)))\n",
    "        out = self.relu1(self.bn1(self.fc1(out)))\n",
    "        out = self.relu2(self.bn2(self.fc2(out)))\n",
    "        out = self.out(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6bfa66-0786-48b6-a2c7-62a94d5ae948",
   "metadata": {},
   "source": [
    "## 3. Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22e7ec2a-4aa2-4c9c-89ae-09249e4138c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f955e390ad0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_gpus = torch.cuda.device_count()\n",
    "print(num_of_gpus)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beeb67fa-cc4e-40c2-be6e-47391d8b0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_loss_plot(losses, title=\"Training loss\", xlabel=\"Iterations\", ylabel=\"Loss\"):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21b349be-f48a-4668-a3e4-0330a4f9b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, val_loader, criterion, learning_rate):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate[0])\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        # Adjust learning rate at epoch 100\n",
    "        if epoch+1 == 50:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = learning_rate[1]\n",
    "                print(f\"Learning rate changed to {param_group['lr']} at epoch {epoch+1}\")\n",
    "        \n",
    "        # Adjust learning rate at epoch 150\n",
    "        if epoch+1 == 75:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = learning_rate[2]\n",
    "                print(f\"Learning rate changed to {param_group['lr']} at epoch {epoch+1}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        print(f\"Epoch [{epoch+1}/{100}], \"\n",
    "              f\"Train Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "              f\"Val Accuracy: {val_acc*100:.2f}%\")\n",
    "        \n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c8d46-9122-46b1-82b2-b32a2cf5ba20",
   "metadata": {},
   "source": [
    "## 4. Define Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "751b60f8-018d-480f-9a95-723d5c185904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    test_acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57ffab9-dbaa-432a-a771-5873971b4bfe",
   "metadata": {},
   "source": [
    "## 5. Train BNN for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f7adc89-8b0f-474c-9f0a-4fd4908f7481",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [64, 128, 256]\n",
    "learning_rates = [[0.0100, 0.0010, 0.0010],\n",
    "                  [0.0010, 0.0010, 0.0010],\n",
    "                  [0.0010, 0.0010, 0.0001],\n",
    "                  [0.0010, 0.0005, 0.0001]\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee8b6634-e421-4c17-82cc-6acc24d6eb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch_size=64, learning_rate=[0.001, 0.001, 0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuhao/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/torch/_tensor.py:1362: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392022560/work/c10/core/TensorImpl.h:1900.)\n",
      "  return super().rename(names)\n",
      "/home/yuhao/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/brevitas/nn/quant_linear.py:69: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392022560/work/torch/csrc/utils/python_arg_parser.cpp:368.)\n",
      "  output_tensor = linear(x, quant_weight, quant_bias)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 0.2180, Val Accuracy: 96.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuhao/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/brevitas/nn/quant_linear.py:69: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392022560/work/torch/csrc/utils/python_arg_parser.cpp:368.)\n",
      "  output_tensor = linear(x, quant_weight, quant_bias)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train and validate\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Update best parameters\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_acc \u001b[38;5;241m>\u001b[39m best_acc:\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_loader, val_loader, criterion, learning_rate)\u001b[0m\n\u001b[1;32m      9\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m, in \u001b[0;36mBiKA_MNIST.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput(out)\n\u001b[1;32m     25\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu0(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn0(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc0(out)))\n\u001b[0;32m---> 26\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     27\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(out)))\n\u001b[1;32m     28\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/brevitas/nn/quant_linear.py:66\u001b[0m, in \u001b[0;36mQuantLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Union[Tensor, QuantTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, QuantTensor]:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/brevitas/nn/quant_layer.py:158\u001b[0m, in \u001b[0;36mQuantWeightBiasInputOutputLayer.forward_impl\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     quant_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m quant_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_quant(output_tensor)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpack_output(quant_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/brevitas/nn/quant_linear.py:69\u001b[0m, in \u001b[0;36mQuantLinear.inner_forward_impl\u001b[0;34m(self, x, quant_weight, quant_bias)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, quant_weight: Tensor, quant_bias: Optional[Tensor]):\n\u001b[0;32m---> 69\u001b[0m     output_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_tensor\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/brevitas/quant_tensor/int_quant_tensor.py:50\u001b[0m, in \u001b[0;36mIntQuantTensor.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m INT_QUANT_TENSOR_FN_HANDLER:\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mINT_QUANT_TENSOR_FN_HANDLER\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m QUANT_TENSOR_FN_HANDLER:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m QUANT_TENSOR_FN_HANDLER[func](\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/brevitas/quant_tensor/int_torch_handler.py:71\u001b[0m, in \u001b[0;36mlinear_handler\u001b[0;34m(quant_input, quant_weight, bias, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;129m@implements_int_qt\u001b[39m(F\u001b[38;5;241m.\u001b[39mlinear)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlinear_handler\u001b[39m(quant_input, quant_weight, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 71\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mquant_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/brevitas/quant_tensor/int_torch_handler.py:198\u001b[0m, in \u001b[0;36mquant_layer\u001b[0;34m(fn, quant_input, quant_weight, bias, *args, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m         output_bit_width \u001b[38;5;241m=\u001b[39m output_bit_width \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_output_quant_tensor:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(quant_input, IntQuantTensor) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    197\u001b[0m         (quant_input\u001b[38;5;241m.\u001b[39mzero_point \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39many()) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(quant_weight, IntQuantTensor) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m                                                    \u001b[43m(\u001b[49m\u001b[43mquant_weight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_point\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    199\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing zero point of output accumulator not supported yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    200\u001b[0m         compute_output_quant_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "best_params = None\n",
    "\n",
    "for batch_size, learning_rate in product(batch_sizes, learning_rates):\n",
    "    print(f\"Training with batch_size={batch_size}, learning_rate={learning_rate}\")\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(data_valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize the model, loss, and optimizer\n",
    "    model = BiKA_MNIST().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train and validate\n",
    "    val_acc = train_and_validate(model, train_loader, val_loader, criterion, learning_rate)\n",
    "\n",
    "    # Update best parameters\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_params = (batch_size, learning_rate)\n",
    "\n",
    "print(f\"Best Accuracy: {best_acc*100:.2f}%\")\n",
    "print(f\"Best Parameters: Batch Size={best_params[0]}, Learning Rate={best_params[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16633ed4-f117-4299-9b19-8b456704b1ca",
   "metadata": {},
   "source": [
    "## 6. Evaluate BNN for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5648bc9-c478-47d1-ab86-912f11108936",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(data_train, batch_size=best_params[0], shuffle=True)\n",
    "val_loader = DataLoader(data_valid, batch_size=best_params[0], shuffle=False)\n",
    "test_loader = DataLoader(data_test, batch_size=best_params[0], shuffle=False)\n",
    "\n",
    "model = BiKA_MNIST().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_and_validate(model, train_loader, val_loader, criterion, best_params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e7729e-d37e-4de7-88b4-19316eacde4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best Validation Accuracy: {best_acc*100:.2f}%\")\n",
    "print(f\"Best Parameters: Batch Size={best_params[0]}, Learning Rate={best_params[1]}\")\n",
    "\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a103e389-5d81-4916-ba80-e336ce91aa5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
