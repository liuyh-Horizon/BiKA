{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4aaa43d-21ca-457a-affd-82b2d289f818",
   "metadata": {},
   "source": [
    "# Try Tiny BNN with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8ad8c1-45ae-4429-861a-cd06e97010f8",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f1b71b-6b55-44a6-b75a-83cc13602bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e32827cb-5c2d-4750-82b3-75c0e160c3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "00%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9912422/9912422 [00:02<00:00, 4836582.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "00%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 258157.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "00%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1648877/1648877 [00:00<00:00, 1985722.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<00:00, 2620069.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_data_train = torchvision.datasets.MNIST('./data/', \n",
    "                                        train=True, download=True,\n",
    "                                        transform=torchvision.transforms.Compose\n",
    "                                        ([\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "                                        ]))\n",
    "\n",
    "# Split the dataset into training and validation subsets\n",
    "train_size = int(0.8 * len(full_data_train))\n",
    "val_size = len(full_data_train) - train_size\n",
    "data_train, data_valid = random_split(full_data_train, [train_size, val_size])\n",
    "\n",
    "data_test = torchvision.datasets.MNIST('./data/', \n",
    "                                       train=False, download=True,\n",
    "                                       transform=torchvision.transforms.Compose\n",
    "                                       ([\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "                                       ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b41f69-98bd-4715-bb09-323e6fb749d3",
   "metadata": {},
   "source": [
    "## 2. Define MLP structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19753678-871a-469c-b544-c79048d99462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e1d3ae1-af90-4a5d-8d1c-f54a448fb719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brevitas.nn as qnn\n",
    "from brevitas.nn import QuantLinear, QuantReLU, QuantConv2d\n",
    "from brevitas.quant.binary import SignedBinaryActPerTensorConst\n",
    "from brevitas.quant.binary import SignedBinaryWeightPerTensorConst\n",
    "from brevitas.inject.enum import QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fffae836-5a45-4667-a622-b71b7b79eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1*28*28      \n",
    "hidden1 = 64      \n",
    "hidden2 = 64\n",
    "hidden3 = 64\n",
    "num_classes = 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e99d4da-52ea-4f61-88a3-6d1915f948c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BiKA_MNIST(Module):\n",
    "    def __init__(self):\n",
    "        super(BiKA_MNIST, self).__init__()\n",
    "        \n",
    "        self.input = qnn.QuantIdentity(quant_type='int', scaling_impl_type='const', bit_width=8, min_val=-128.0, max_val=127.0, return_quant_tensor=True)\n",
    "        \n",
    "        self.fc0   = qnn.QuantLinear(input_size, hidden1, weight_bit_width=8, weight_quant_type=QuantType.INT, bias=False)\n",
    "        self.bn0   = nn.BatchNorm1d(hidden1)\n",
    "        self.relu0 = qnn.QuantReLU(bit_width=8, return_quant_tensor=True)\n",
    "        \n",
    "        self.fc1   = qnn.QuantLinear(hidden1, hidden2, weight_bit_width=8, weight_quant_type=QuantType.INT, bias=False)\n",
    "        self.bn1   = nn.BatchNorm1d(hidden2)\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=8, return_quant_tensor=True)\n",
    "        \n",
    "        self.fc2   = qnn.QuantLinear(hidden2, hidden3, weight_bit_width=8, weight_quant_type=QuantType.INT, bias=False)\n",
    "        self.bn2   = nn.BatchNorm1d(hidden3)\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=8, return_quant_tensor=True)\n",
    "        \n",
    "        self.out   = qnn.QuantLinear(hidden3, num_classes, weight_bit_width=8, weight_quant_type=QuantType.INT, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = x.reshape(x.shape[0], -1)\n",
    "        out = self.input(out)\n",
    "        out = self.relu0(self.bn0(self.fc0(out)))\n",
    "        out = self.relu1(self.bn1(self.fc1(out)))\n",
    "        out = self.relu2(self.bn2(self.fc2(out)))\n",
    "        out = self.out(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6bfa66-0786-48b6-a2c7-62a94d5ae948",
   "metadata": {},
   "source": [
    "## 3. Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22e7ec2a-4aa2-4c9c-89ae-09249e4138c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efb5ed20ad0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_gpus = torch.cuda.device_count()\n",
    "print(num_of_gpus)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beeb67fa-cc4e-40c2-be6e-47391d8b0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_loss_plot(losses, title=\"Training loss\", xlabel=\"Iterations\", ylabel=\"Loss\"):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21b349be-f48a-4668-a3e4-0330a4f9b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, val_loader, criterion, learning_rate):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate[0])\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        # Adjust learning rate at epoch 100\n",
    "        if epoch+1 == 50:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = learning_rate[1]\n",
    "                print(f\"Learning rate changed to {param_group['lr']} at epoch {epoch+1}\")\n",
    "        \n",
    "        # Adjust learning rate at epoch 150\n",
    "        if epoch+1 == 75:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = learning_rate[2]\n",
    "                print(f\"Learning rate changed to {param_group['lr']} at epoch {epoch+1}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        print(f\"Epoch [{epoch+1}/{100}], \"\n",
    "              f\"Train Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "              f\"Val Accuracy: {val_acc*100:.2f}%\")\n",
    "        \n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c8d46-9122-46b1-82b2-b32a2cf5ba20",
   "metadata": {},
   "source": [
    "## 4. Define Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "751b60f8-018d-480f-9a95-723d5c185904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    test_acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57ffab9-dbaa-432a-a771-5873971b4bfe",
   "metadata": {},
   "source": [
    "## 5. Train BNN for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f7adc89-8b0f-474c-9f0a-4fd4908f7481",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [64, 128, 256]\n",
    "learning_rates = [[0.0100, 0.0010, 0.0010],\n",
    "                  [0.0010, 0.0010, 0.0010],\n",
    "                  [0.0010, 0.0010, 0.0001],\n",
    "                  [0.0010, 0.0005, 0.0001]\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee8b6634-e421-4c17-82cc-6acc24d6eb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch_size=64, learning_rate=[0.001, 0.001, 0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuhao/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/torch/_tensor.py:1362: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392022560/work/c10/core/TensorImpl.h:1900.)\n",
      "  return super().rename(names)\n",
      "/home/yuhao/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/brevitas/nn/quant_linear.py:69: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392022560/work/torch/csrc/utils/python_arg_parser.cpp:368.)\n",
      "  output_tensor = linear(x, quant_weight, quant_bias)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 0.3308, Val Accuracy: 96.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuhao/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/brevitas/nn/quant_linear.py:69: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392022560/work/torch/csrc/utils/python_arg_parser.cpp:368.)\n",
      "  output_tensor = linear(x, quant_weight, quant_bias)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train and validate\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Update best parameters\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_acc \u001b[38;5;241m>\u001b[39m best_acc:\n",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_loader, val_loader, criterion, learning_rate)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[1;32m      7\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      9\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas_lyh/lib/python3.9/site-packages/torchvision/datasets/mnist.py:142\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    138\u001b[0m img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[index])\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "best_params = None\n",
    "\n",
    "for batch_size, learning_rate in product(batch_sizes, learning_rates):\n",
    "    print(f\"Training with batch_size={batch_size}, learning_rate={learning_rate}\")\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(data_valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize the model, loss, and optimizer\n",
    "    model = BiKA_MNIST().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train and validate\n",
    "    val_acc = train_and_validate(model, train_loader, val_loader, criterion, learning_rate)\n",
    "\n",
    "    # Update best parameters\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_params = (batch_size, learning_rate)\n",
    "\n",
    "print(f\"Best Accuracy: {best_acc*100:.2f}%\")\n",
    "print(f\"Best Parameters: Batch Size={best_params[0]}, Learning Rate={best_params[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16633ed4-f117-4299-9b19-8b456704b1ca",
   "metadata": {},
   "source": [
    "## 6. Evaluate BNN for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5648bc9-c478-47d1-ab86-912f11108936",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(data_train, batch_size=best_params[0], shuffle=True)\n",
    "val_loader = DataLoader(data_valid, batch_size=best_params[0], shuffle=False)\n",
    "test_loader = DataLoader(data_test, batch_size=best_params[0], shuffle=False)\n",
    "\n",
    "model = BiKA_MNIST().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_and_validate(model, train_loader, val_loader, criterion, best_params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e7729e-d37e-4de7-88b4-19316eacde4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best Validation Accuracy: {best_acc*100:.2f}%\")\n",
    "print(f\"Best Parameters: Batch Size={best_params[0]}, Learning Rate={best_params[1]}\")\n",
    "\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a103e389-5d81-4916-ba80-e336ce91aa5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
