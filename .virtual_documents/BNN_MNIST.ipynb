import torchvision
from torchvision import datasets
from torchvision import transforms
from torch.autograd import Variable


data_train = torchvision.datasets.MNIST('./data/', 
                                        train=True, download=True,
                                        transform=torchvision.transforms.Compose
                                        ([
                                            torchvision.transforms.ToTensor(),
                                            #torchvision.transforms.Normalize((0.1307,), (0.3081,))
                                            torchvision.transforms.Normalize((0.5,), (0.5,))
                                        ]))
data_test = torchvision.datasets.MNIST('./data/', 
                                       train=False, download=True,
                                       transform=torchvision.transforms.Compose
                                       ([
                                            torchvision.transforms.ToTensor(),
                                            #torchvision.transforms.Normalize((0.1307,), (0.3081,))
                                            torchvision.transforms.Normalize((0.5,), (0.5,))
                                       ]))


import torch

batch_size_train = 64
batch_size_test = 1000

data_loader_train = torch.utils.data.DataLoader(dataset=data_train,
                                                batch_size=batch_size_train, 
                                                shuffle=True)

data_loader_test = torch.utils.data.DataLoader(dataset=data_test,
                                               batch_size=batch_size_test, 
                                               shuffle=True)


examples = enumerate(data_loader_train)
batch_idx, (example_data, example_targets) = next(examples)
print(example_data[0][0])
example_data[0][0].shape


import matplotlib.pyplot as plt

fig = plt.figure()
for i in range(6):
  plt.subplot(2,3,i+1)
  plt.tight_layout()
  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')
  plt.title("Ground Truth: {}".format(example_targets[i]))
  plt.xticks([])
  plt.yticks([])
fig


input_size = 1*28*28      
hidden1 = 64      
hidden2 = 64
hidden3 = 64
weight_bit_width = 1
act_bit_width = 1
num_classes = 10  


import brevitas.nn as qnn
from brevitas.nn import QuantLinear, QuantReLU
import torch.nn as nn
from torch.nn import Module
from brevitas.quant.binary import SignedBinaryActPerTensorConst
from brevitas.quant.binary import SignedBinaryWeightPerTensorConst
from brevitas.inject.enum import QuantType

# Setting seeds for reproducibility
torch.manual_seed(0)

class TFC_W1A1(Module):
    def __init__(self):
        super(TFC_W1A1, self).__init__()
        
        self.input = qnn.QuantIdentity(
                         quant_type='binary',
                         scaling_impl_type='const',
                         bit_width=1,
                         min_val=-1.0,
                         max_val=1.0, 
                         return_quant_tensor=True
                     )
        
        self.fc1   = qnn.QuantLinear(
                         input_size, 
                         hidden1, 
                         weight_bit_width=weight_bit_width,
                         weight_quant_type=QuantType.BINARY,
                         bias=False
                     )
        
        self.bn1   = nn.BatchNorm1d(hidden1)
        self.relu1 = qnn.QuantReLU(
                         bit_width=act_bit_width, 
                         return_quant_tensor=True
                     )
        
        self.fc2   = qnn.QuantLinear(
                         hidden1, 
                         hidden2, 
                         weight_bit_width=weight_bit_width,
                         weight_quant_type=QuantType.BINARY,
                         bias=False
                     )
        self.bn2   = nn.BatchNorm1d(hidden2)
        self.relu2 = qnn.QuantReLU(
                         bit_width=act_bit_width, 
                         return_quant_tensor=True
                     )
        
        
        self.fc3   = qnn.QuantLinear(
                         hidden2, 
                         hidden3, 
                         weight_bit_width=weight_bit_width,
                         weight_quant_type=QuantType.BINARY,
                         bias=False
                     )
        self.bn3   = nn.BatchNorm1d(hidden3)
        self.relu3 = qnn.QuantReLU(
                         bit_width=act_bit_width, 
                         return_quant_tensor=True
                     )
        
        
        self.out   = qnn.QuantLinear(
                         hidden3, 
                         num_classes, 
                         weight_bit_width=weight_bit_width,
                         weight_quant_type=QuantType.BINARY,
                         bias=False
                     )
        
        self.accu_min_1 = 65535.0
        self.accu_max_1 = 0.0
        
        self.bn_min_1 = 65535.0
        self.bn_max_1 = 0.0
        
        self.out_min_1 = 65535.0
        self.out_max_1 = 0.0
        
        self.accu_min_2 = 65535.0
        self.accu_max_2 = 0.0
        
        self.bn_min_2 = 65535.0
        self.bn_max_2 = 0.0
        
        self.out_min_2 = 65535.0
        self.out_max_2 = 0.0
        
        self.accu_min_3 = 65535.0
        self.accu_max_3 = 0.0
        
        self.bn_min_3 = 65535.0
        self.bn_max_3 = 0.0
        
        self.out_min_3 = 65535.0
        self.out_max_3 = 0.0

    def forward(self, x, en):
        
        out = x.reshape(x.shape[0], -1)
        out = self.input(out)
        #out = self.relu1(self.bn1(self.fc1(out)))
        out=self.fc1(out)
        if (en==True):
            if (torch.max(out)>self.accu_max_1):
                self.accu_max_1 = torch.max(out)
            if (torch.min(out)<self.accu_min_1):
                self.accu_min_1 = torch.min(out)
        out=self.bn1(out)
        if (en==True):
            if (torch.max(out)>self.bn_max_1):
                self.bn_max_1 = torch.max(out)
            if (torch.min(out)<self.bn_min_1):
                self.bn_min_1 = torch.min(out)
        out=self.relu1(out)
        if (en==True):
            if (torch.max(out)>self.out_max_1):
                self.out_max_1 = torch.max(out)
            if (torch.min(out)<self.out_min_1):
                self.out_min_1 = torch.min(out)
        #out = self.relu2(self.bn2(self.fc2(out)))
        out=self.fc2(out)
        if (en==True):
            if (torch.max(out)>self.accu_max_2):
                self.accu_max_2 = torch.max(out)
            if (torch.min(out)<self.accu_min_2):
                self.accu_min_2 = torch.min(out)
        out=self.bn2(out)
        if (en==True):
            if (torch.max(out)>self.bn_max_2):
                self.bn_max_2 = torch.max(out)
            if (torch.min(out)<self.bn_min_2):
                self.bn_min_2 = torch.min(out)
        out=self.relu2(out)
        if (en==True):
            if (torch.max(out)>self.out_max_2):
                self.out_max_2 = torch.max(out)
            if (torch.min(out)<self.out_min_2):
                self.out_min_2 = torch.min(out)
        #out = self.relu3(self.bn3(self.fc3(out)))
        out=self.fc3(out)
        if (en==True):
            if (torch.max(out)>self.accu_max_3):
                self.accu_max_3 = torch.max(out)
            if (torch.min(out)<self.accu_min_3):
                self.accu_min_3 = torch.min(out)
        out=self.bn3(out)
        if (en==True):
            if (torch.max(out)>self.bn_max_3):
                self.bn_max_3 = torch.max(out)
            if (torch.min(out)<self.bn_min_3):
                self.bn_min_3 = torch.min(out)
        out=self.relu3(out)
        if (en==True):
            if (torch.max(out)>self.out_max_3):
                self.out_max_3 = torch.max(out)
            if (torch.min(out)<self.out_min_3):
                self.out_min_3 = torch.min(out)
        out = self.out(out)
        return out
   
model = TFC_W1A1()

#class TFC_W1A1(Module):
#    def __init__(self):
#        super(TFC_W1A1, self).__init__()
#        
#        self.input = qnn.QuantIdentity(
#                         quant_type=SignedBinaryActPerTensorConst, 
#                         return_quant_tensor=True
#                     )
#        
#        self.fc1   = qnn.QuantLinear(
#                         input_size, 
#                         hidden1, 
#                         input_quant=None,
#                         bias_quant=None,
#                         weight_quant=SignedBinaryWeightPerTensorConst, 
#                         output_quant=None,
#                         bias=True
#                     )
#        
#        self.bn1   = nn.BatchNorm1d(hidden1)
#        self.dp1   = nn.Dropout(0.5)
#        self.relu1 = qnn.QuantReLU(
#                         input_quant=None,
#                         act_quant=SignedBinaryActPerTensorConst, 
#                         return_quant_tensor=True
#                     )
#        
#        self.fc2   = qnn.QuantLinear(
#                         hidden1, 
#                         hidden2, 
#                         input_quant=None,
#                         bias_quant=None,
#                         weight_quant=SignedBinaryWeightPerTensorConst, 
#                         output_quant=None,
#                         bias=True
#                     )
#        self.bn2   = nn.BatchNorm1d(hidden2)
#        self.dp2   = nn.Dropout(0.5)
#        self.relu2 = qnn.QuantReLU(
#                         input_quant=None,
#                         act_quant=SignedBinaryActPerTensorConst, 
#                         return_quant_tensor=True
#                     )
#        
#        
#        self.fc3   = qnn.QuantLinear(
#                         hidden2, 
#                         hidden3, 
#                         input_quant=None,
#                         bias_quant=None,
#                         weight_quant=SignedBinaryWeightPerTensorConst, 
#                         output_quant=None,
#                         bias=True
#                     )
#        self.bn3   = nn.BatchNorm1d(hidden3)
#        self.dp3   = nn.Dropout(0.5)
#        self.relu3 = qnn.QuantReLU(
#                         input_quant=None,
#                         act_quant=SignedBinaryActPerTensorConst, 
#                         return_quant_tensor=True
#                     )
#        
#        
#        self.out   = qnn.QuantLinear(
#                         hidden3, 
#                         num_classes, 
#                         input_quant=None,
#                         bias_quant=None,
#                         weight_quant=SignedBinaryWeightPerTensorConst, 
#                         output_quant=None,
#                         bias=True
#                     )
#
#    def forward(self, x):
#        out = x.reshape(x.shape[0], -1)
#        out = self.input(out)
#        out = self.relu1(self.dp1(self.bn1(self.fc1(out))))
#        out = self.relu2(self.dp2(self.bn2(self.fc2(out))))
#        out = self.relu3(self.dp3(self.bn3(self.fc3(out))))
#        out = self.out(out)
#        return out
#   
#model = TFC_W1A1()

#model = nn.Sequential(
#      nn.Flatten(),
#      QuantLinear(input_size, hidden1, bias=True, weight_bit_width=weight_bit_width),
#      nn.BatchNorm1d(hidden1),
#      nn.Dropout(0.5),
#      QuantReLU(bit_width=act_bit_width),
#      QuantLinear(hidden1, hidden2, bias=True, weight_bit_width=weight_bit_width),
#      nn.BatchNorm1d(hidden2),
#      nn.Dropout(0.5),
#      QuantReLU(bit_width=act_bit_width),
#      QuantLinear(hidden2, hidden3, bias=True, weight_bit_width=weight_bit_width),
#      nn.BatchNorm1d(hidden3),
#      nn.Dropout(0.5),
#      QuantReLU(bit_width=act_bit_width),
#      QuantLinear(hidden3, num_classes, bias=True, weight_bit_width=weight_bit_width)
#)

#class LowPrecisionMLP(Module):
#    def __init__(self):
#        super(LowPrecisionMLP, self).__init__()
#        
#        self.fc1   = qnn.QuantLinear(input_size, hidden1, bias=True, weight_bit_width=weight_bit_width)
#        self.bn1   = nn.BatchNorm1d(hidden1)
#        self.dp1   = nn.Dropout(0.5)
#        self.relu1 = qnn.QuantReLU(bit_width=act_bit_width)
#        
#        self.fc2   = qnn.QuantLinear(hidden1, hidden2, bias=True, weight_bit_width=weight_bit_width)
#        self.bn2   = nn.BatchNorm1d(hidden2)
#        self.dp2   = nn.Dropout(0.5)
#        self.relu2 = qnn.QuantReLU(bit_width=act_bit_width)
#        
#        self.fc3   = qnn.QuantLinear(hidden2, hidden3, bias=True, weight_bit_width=weight_bit_width)
#        self.bn3   = nn.BatchNorm1d(hidden3)
#        self.dp3   = nn.Dropout(0.5)
#        self.relu3 = qnn.QuantReLU(bit_width=act_bit_width)
#        
#        self.out   = qnn.QuantLinear(hidden3, num_classes, bias=True, weight_bit_width=weight_bit_width)
#
#    def forward(self, x):
#        out = x.reshape(x.shape[0], -1)
#        out = self.relu1(self.dp1(self.bn1(self.fc1(out))))
#        out = self.relu2(self.dp2(self.bn2(self.fc2(out))))
#        out = self.relu3(self.dp3(self.bn3(self.fc3(out))))
#        out = self.out(out)
#        return out
    
#model = LowPrecisionMLP()


def train(model, train_loader, optimizer, criterion):
    losses = []
    # ensure model is in training mode
    model.train()    
    
    for i, data in enumerate(train_loader, 0):        
        inputs, target = data
        #inputs, target = inputs.cuda(), target.cuda()
        inputs, target = Variable(inputs), Variable(target)
        
        outputs = model(inputs, False)
        _,pred = torch.max(outputs.data,1)
        
        optimizer.zero_grad()
        loss = criterion(outputs,target)
 
        loss.backward()
        optimizer.step()
        
        # keep track of loss value
        losses.append(loss.data.numpy()) 
           
    return losses


import torch
from sklearn.metrics import accuracy_score

def test(model, test_loader, enable=False):    
    # ensure model is in eval mode
    model.eval() 
    y_true = []
    y_pred = []
   
    with torch.no_grad():
        for data in test_loader:
            inputs, target = data
            #inputs, target = inputs.cuda(), target.cuda()
            inputs, target = Variable(inputs),Variable(target)
            output = model(inputs, enable)
            #output = torch.sigmoid(output_orig)  
            _,pred = torch.max(output,1)
            # compare against a threshold of 0.5 to generate 0/1
            y_true.extend(target.tolist()) 
            y_pred.extend(pred.reshape(-1).tolist())
        
    return accuracy_score(y_true, y_pred)


num_epochs = 10
learn_rate = 0.001 

def display_loss_plot(losses, title="Training loss", xlabel="Iterations", ylabel="Loss"):
    x_axis = [i for i in range(len(losses))]
    plt.plot(x_axis,losses)
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.show()


# loss criterion and optimizer
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate, betas=(0.9, 0.999))


import numpy as np
from sklearn.metrics import accuracy_score
from tqdm import tqdm, trange

# Setting seeds for reproducibility
torch.manual_seed(0)
np.random.seed(0)

running_loss = []
running_test_acc = []
t = trange(num_epochs, desc="Training loss", leave=True)

for epoch in t:
        loss_epoch = train(model, data_loader_train, optimizer, criterion)
        test_acc = test(model, data_loader_test)
        t.set_description("Training loss = %f test accuracy = %f" % (np.mean(loss_epoch), test_acc))
        t.refresh() # to show immediately the update           
        running_loss.append(loss_epoch)
        running_test_acc.append(test_acc)


%matplotlib inline
import matplotlib.pyplot as plt

loss_per_epoch = [np.mean(loss_per_epoch) for loss_per_epoch in running_loss]
display_loss_plot(loss_per_epoch)


acc_per_epoch = [np.mean(acc_per_epoch) for acc_per_epoch in running_test_acc]
display_loss_plot(acc_per_epoch, title="Test accuracy", ylabel="Accuracy [%]")


test(model, data_loader_test, True)


print("Layer 1:\n")
print("    Accumulator Output Range: [", model.accu_min_1, ", ", model.accu_max_1, "]\n")
print("    BN Output Range: [", model.bn_min_1, ", ", model.bn_max_1, "]\n")
print("    Multi-Threshold Output Range: [", model.out_min_1, ", ", model.out_max_1, "]\n")

print("Layer 2:\n")
print("    Accumulator Output Range: [", model.accu_min_2, ", ", model.accu_max_2, "]\n")
print("    BN Output Range: [", model.bn_min_2, ", ", model.bn_max_2, "]\n")
print("    Multi-Threshold Output Range: [", model.out_min_2, ", ", model.out_max_2, "]\n")

print("Layer 3:\n")
print("    Accumulator Output Range: [", model.accu_min_3, ", ", model.accu_max_3, "]\n")
print("    BN Output Range: [", model.bn_min_3, ", ", model.bn_max_3, "]\n")
print("    Multi-Threshold Output Range: [", model.out_min_3, ", ", model.out_max_3, "]\n")


torch.save(model.state_dict(), "./model/torch/tfc_w1a1.pth")


import brevitas.onnx as bo
from brevitas.quant_tensor import QuantTensor

ready_model_filename = "./model/onnx/brevitas/brevitas_tfc_w1a1.onnx"
input_shape = (1, 28, 28)

#Move to CPU before export
model.cpu()

# Export to ONNX
bo.export_finn_onnx(
#    model, export_path=ready_model_filename, input_t=input_qt,
    model, export_path=ready_model_filename, input_shape=input_shape,
                     export_params=True, do_constant_folding=True,
                     input_names=['input'], output_names=['output']
)

print("Model saved to %s" % ready_model_filename)





from finn.util.visualization import showInNetron

showInNetron(ready_model_filename)
