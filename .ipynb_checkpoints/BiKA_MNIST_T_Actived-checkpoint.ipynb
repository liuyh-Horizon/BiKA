{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9716d63-eedf-4466-abe4-19ba8a633fb5",
   "metadata": {},
   "source": [
    "# Define BiKA Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f42cc2af-891e-488f-8a2f-9c4808a378d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a09ecd-5136-469a-952a-0e86ae3f8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output during inference: tensor([ 1., -1., -1.,  1.], grad_fn=<CustomSignFunctionBackward>)\n",
      "Gradient during training: tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "class CustomSignFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Save the input for backward computation\n",
    "        ctx.save_for_backward(input)\n",
    "        # Output +1 for input > 0, else -1 (including for input == 0)\n",
    "        return torch.where(input > 0, torch.tensor(1.0, device=input.device), torch.tensor(-1.0, device=input.device))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve the input saved in the forward pass\n",
    "        input, = ctx.saved_tensors\n",
    "        # Gradient of the input is the same as the gradient output (STE)\n",
    "        grad_input = grad_output.clone()\n",
    "        # Pass the gradient only where input was non-zero, otherwise set it to 0\n",
    "        grad_input[input.abs() > 0] = grad_output[input.abs() > 0]\n",
    "        return grad_input\n",
    "\n",
    "# Wrapper class for convenience\n",
    "class CustomSignActivation(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomSignActivation, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return CustomSignFunction.apply(input)\n",
    "\n",
    "# Example usage:\n",
    "sign_activation = CustomSignActivation()\n",
    "\n",
    "# Test the forward pass\n",
    "x = torch.tensor([2.0, -3.0, 0.0, 1.5], requires_grad=True)\n",
    "output = sign_activation(x)\n",
    "print(\"Output during inference:\", output)\n",
    "\n",
    "# Test the backward pass (gradient computation during training)\n",
    "loss = output.sum()  # Just an example loss\n",
    "loss.backward()\n",
    "print(\"Gradient during training:\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35ab553d-22ba-4ce6-a69e-d131fb55c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiKALinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(BiKALinear, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.sign = CustomSignActivation()\n",
    "            \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Expand the input to match the bias shape for broadcasting\n",
    "        # x is of shape (batch_size, in_features)\n",
    "        # Expand bias matrix to (batch_size, out_features, in_features)\n",
    "        x = x.unsqueeze(1) + self.bias.unsqueeze(0)\n",
    "        \n",
    "        # Perform element-wise multiplication with weights\n",
    "        x = x * self.weight.unsqueeze(0)\n",
    "        \n",
    "        # Apply sign function: -1 for negative and 0, 1 for positive\n",
    "        x = self.sign(x)\n",
    "        \n",
    "        # Sum the thresholded products along the input features dimension\n",
    "        x = torch.sum(x, dim=-1) \n",
    "\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "bika_linear = BiKALinear(in_features=2, out_features=3)\n",
    "input_tensor  = torch.randn(3, 2)  # Batch of 3, 10 input features each\n",
    "output_tensor = bika_linear(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "427c2f6c-096f-4b6a-9b45-3a4379a0aa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4887, -0.4629],\n",
      "        [-0.8644, -0.1726],\n",
      "        [ 1.6447, -0.9748]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor)\n",
    "print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38906c59-f428-4c2e-b4b6-65362ec34888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2861, -0.5992],\n",
      "        [-0.7061,  0.0750],\n",
      "        [-0.4884, -0.3987]], requires_grad=True)\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(bika_linear.weight)\n",
    "print(bika_linear.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "325140eb-8269-413f-af18-5a5845eaedf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.5891, -0.6023],\n",
      "        [ 0.0524,  0.6141],\n",
      "        [ 0.3920,  0.6212]], requires_grad=True)\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(bika_linear.bias)\n",
    "print(bika_linear.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "325f3c88-7bad-4589-b8b9-464f8222af25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0., -2.],\n",
      "        [ 2.,  2.,  0.],\n",
      "        [ 0., -2.,  0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(output_tensor)\n",
    "print(output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aaa43d-21ca-457a-affd-82b2d289f818",
   "metadata": {},
   "source": [
    "# Try Tiny BiKA with MNIST and output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12f1b71b-6b55-44a6-b75a-83cc13602bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e32827cb-5c2d-4750-82b3-75c0e160c3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = torchvision.datasets.MNIST('./data/', \n",
    "                                        train=True, download=True,\n",
    "                                        transform=torchvision.transforms.Compose\n",
    "                                        ([\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            #torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                            torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "                                        ]))\n",
    "data_test = torchvision.datasets.MNIST('./data/', \n",
    "                                       train=False, download=True,\n",
    "                                       transform=torchvision.transforms.Compose\n",
    "                                       ([\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            #torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                            torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "                                       ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "308351c5-ce72-46d9-ad4e-584dfe360b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size_train = 512\n",
    "batch_size_test = 1000\n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(dataset=data_train,\n",
    "                                                batch_size=batch_size_train, \n",
    "                                                shuffle=True)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset=data_test,\n",
    "                                               batch_size=batch_size_test, \n",
    "                                               shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "944df90d-dc16-44f2-b3aa-b3c617c00e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9843, -0.3098,\n",
      "          0.3020, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8039,  0.9843,\n",
      "          0.6392, -0.9451, -1.0000, -1.0000, -0.9216, -0.9765, -0.9059, -0.8039,\n",
      "         -0.8039, -0.8039, -0.8039, -0.8039, -0.9529, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8039,  0.9843,\n",
      "          0.9843,  0.4431,  0.2314,  0.2314,  0.5216,  0.3020,  0.5922,  0.9922,\n",
      "          0.9843,  0.9843,  0.9843,  0.9843,  0.4118, -0.1765, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8039,  0.9843,\n",
      "          0.9843,  0.9843,  0.9843,  0.9843,  0.8431,  0.6078,  0.6078,  0.6157,\n",
      "          0.6078,  0.6078,  0.6078,  0.6941,  0.9843,  0.7647, -0.6314, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8039,  0.9843,\n",
      "          0.9843,  0.9843, -0.0980, -0.4353, -0.6471, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -0.8588,  0.6549,  0.9843,  0.0039, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8588,  0.7725,\n",
      "          0.9843,  0.9843, -0.5294, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000,  0.5216,  0.9843, -0.7176, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.2314,\n",
      "          0.9843,  0.9843, -0.2078, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -0.6078,  0.8980,  0.7647, -0.8118, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.2314,\n",
      "          0.9843,  0.9843,  0.4118, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -0.1529,  0.9843,  0.3176, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6235,\n",
      "          0.9843,  0.9843,  0.9373, -0.6549, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -0.8353,  0.6627,  0.9843,  0.3176, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9843,\n",
      "          0.3569,  0.9843,  0.9843, -0.1686, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -0.6157,  0.9843,  0.9843, -0.5843, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -0.6157,  0.9922,  0.9922,  0.3569, -0.9843, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -0.1137,  0.9922,  0.9686, -0.6392, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -0.6549,  0.9294,  0.9843,  0.6000, -0.8824, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -0.9686,  0.3961,  0.9843, -0.1216, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000,  0.2863,  0.9843,  0.1843, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -0.5765,  0.9843,  0.9843, -0.5294, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -0.8353, -0.3412, -0.8745, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000,  0.2314,  0.9843,  0.9843, -0.5294, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000,  0.2314,  0.9843,  0.6471, -0.8667, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -0.8510,  0.8118,  0.9843,  0.2392, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9608,\n",
      "          0.2471,  0.9843,  0.8588, -0.6235, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8980,\n",
      "          0.9843,  0.9843,  0.6078, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.0353,\n",
      "          0.9843,  0.9843, -0.2549, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.0431,\n",
      "          0.9843,  0.2549, -0.9686, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = enumerate(data_loader_train)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data[0][0])\n",
    "example_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b2a95ff-ea7a-4e26-9c24-854d71c77eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fffae836-5a45-4667-a622-b71b7b79eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1*28*28      \n",
    "hidden1 = 64      \n",
    "hidden2 = 64\n",
    "hidden3 = 64\n",
    "num_classes = 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e99d4da-52ea-4f61-88a3-6d1915f948c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import brevitas.nn as qnn\n",
    "from brevitas.nn import QuantLinear, QuantReLU\n",
    "from brevitas.inject.enum import QuantType\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class BiKA_MNIST(Module):\n",
    "    def __init__(self):\n",
    "        super(BiKA_MNIST, self).__init__()\n",
    "        \n",
    "        self.fc0   = BiKALinear(in_features=input_size, out_features=hidden1)\n",
    "        self.relu0 = qnn.QuantReLU(\n",
    "                         bit_width=8, \n",
    "                         return_quant_tensor=True\n",
    "                     )\n",
    "        \n",
    "        self.fc1   = BiKALinear(in_features=hidden1, out_features=hidden2)\n",
    "        self.relu1 = qnn.QuantReLU(\n",
    "                         bit_width=8, \n",
    "                         return_quant_tensor=True\n",
    "                     )\n",
    "        \n",
    "        self.fc2   = BiKALinear(in_features=hidden2, out_features=hidden3)\n",
    "        self.relu2 = qnn.QuantReLU(\n",
    "                         bit_width=8, \n",
    "                         return_quant_tensor=True\n",
    "                     )\n",
    "        \n",
    "        self.out   = BiKALinear(in_features=hidden3, out_features=num_classes)\n",
    "        self.relu3 = qnn.QuantReLU(\n",
    "                         bit_width=8, \n",
    "                         return_quant_tensor=True\n",
    "                     )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = x.reshape(x.shape[0], -1)\n",
    "        \n",
    "        out = self.relu0(self.fc0(out))\n",
    "        out = out._pre_round_int_value\n",
    "        \n",
    "        out = self.relu1(self.fc1(out))\n",
    "        out = out._pre_round_int_value\n",
    "        \n",
    "        out = self.relu2(self.fc2(out))\n",
    "        out = out._pre_round_int_value\n",
    "        \n",
    "        out = self.out(out)\n",
    "        \n",
    "        return out\n",
    "   \n",
    "model = BiKA_MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21b349be-f48a-4668-a3e4-0330a4f9b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    losses = []\n",
    "    # ensure model is in training mode\n",
    "    model.train()    \n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):        \n",
    "        inputs, target = data\n",
    "        #inputs, target = inputs.cuda(), target.cuda()\n",
    "        inputs, target = Variable(inputs), Variable(target)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _,pred = torch.max(outputs.data,1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs,target)\n",
    " \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # keep track of loss value\n",
    "        losses.append(loss.data.numpy()) \n",
    "           \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98efeebc-78b2-484a-92a8-d54ab7375d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def test(model, test_loader):    \n",
    "    # ensure model is in eval mode\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, target = data\n",
    "            #inputs, target = inputs.cuda(), target.cuda()\n",
    "            inputs, target = Variable(inputs),Variable(target)\n",
    "            output = model(inputs)\n",
    "            #output = torch.sigmoid(output_orig)  \n",
    "            _,pred = torch.max(output,1)\n",
    "            # compare against a threshold of 0.5 to generate 0/1\n",
    "            y_true.extend(target.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beeb67fa-cc4e-40c2-be6e-47391d8b0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "learn_rate = 0.001 \n",
    "\n",
    "def display_loss_plot(losses, title=\"Training loss\", xlabel=\"Iterations\", ylabel=\"Loss\"):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fa929fb-5420-4473-98b5-4feaed3e09e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss criterion and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b09afe0-b490-435e-823e-f1e1a7999dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 2.994097 test accuracy = 0.101000:  15%|▏| 3/20 [01:27<08:17, 29\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_49896/1117317652.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training loss = %f test accuracy = %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_49896/3486183246.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Software/Conda_Envs/Brevitas_old/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Software/Conda_Envs/Brevitas_old/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;32m~/Projects/Software/Conda_Envs/Brevitas_old/lib/python3.7/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    251\u001b[0m                                \"of them.\")\n\u001b[1;32m    252\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_49896/3665646097.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mgrad_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Pass the gradient only where input was non-zero, otherwise set it to 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mgrad_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "running_loss = []\n",
    "running_test_acc = []\n",
    "t = trange(num_epochs, desc=\"Training loss\", leave=True)\n",
    "\n",
    "for epoch in t:\n",
    "        loss_epoch = train(model, data_loader_train, optimizer, criterion)\n",
    "        test_acc = test(model, data_loader_test)\n",
    "        t.set_description(\"Training loss = %f test accuracy = %f\" % (np.mean(loss_epoch), test_acc))\n",
    "        t.refresh() # to show immediately the update           \n",
    "        running_loss.append(loss_epoch)\n",
    "        running_test_acc.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f7e524-799e-403d-a9b9-6a4b26fa1910",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_per_epoch = [np.mean(loss_per_epoch) for loss_per_epoch in running_loss]\n",
    "display_loss_plot(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e88e03b-ace4-4ae2-965a-8e85106ff5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_per_epoch = [np.mean(acc_per_epoch) for acc_per_epoch in running_test_acc]\n",
    "display_loss_plot(acc_per_epoch, title=\"Test accuracy\", ylabel=\"Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5648bc9-c478-47d1-ab86-912f11108936",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, data_loader_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
