{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cfd9c06-5604-49a2-9fdc-4217b483a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d21be32-53be-49ff-99af-d7a5388e2afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output during inference: tensor([ 1., -1., -1.,  1.], grad_fn=<CustomSignFunctionBackward>)\n",
      "Gradient during training: tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "class CustomSignFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Save the input for backward computation\n",
    "        ctx.save_for_backward(input)\n",
    "        # Output +1 for input > 0, else -1 (including for input == 0)\n",
    "        return torch.where(input > 0, torch.tensor(1.0, device=input.device), torch.tensor(-1.0, device=input.device))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve the input saved in the forward pass\n",
    "        input, = ctx.saved_tensors\n",
    "        # Gradient of the input is the same as the gradient output (STE)\n",
    "        grad_input = grad_output.clone()\n",
    "        # Pass the gradient only where input was non-zero, otherwise set it to 0\n",
    "        grad_input[input.abs() > 0] = grad_output[input.abs() > 0]\n",
    "        return grad_input\n",
    "\n",
    "# Wrapper class for convenience\n",
    "class CustomSignActivation(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomSignActivation, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return CustomSignFunction.apply(input)\n",
    "\n",
    "# Example usage:\n",
    "sign_activation = CustomSignActivation()\n",
    "\n",
    "# Test the forward pass\n",
    "x = torch.tensor([2.0, -3.0, 0.0, 1.5], requires_grad=True)\n",
    "output = sign_activation(x)\n",
    "print(\"Output during inference:\", output)\n",
    "\n",
    "# Test the backward pass (gradient computation during training)\n",
    "loss = output.sum()  # Just an example loss\n",
    "loss.backward()\n",
    "print(\"Gradient during training:\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c04babe-edac-4d60-b458-2550bdddc672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiKALinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(BiKALinear, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.sign = CustomSignActivation()\n",
    "            \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Expand the input to match the bias shape for broadcasting\n",
    "        # x is of shape (batch_size, in_features)\n",
    "        # Expand bias matrix to (batch_size, out_features, in_features)\n",
    "        x = x.unsqueeze(1) + self.bias.unsqueeze(0)\n",
    "        \n",
    "        # Perform element-wise multiplication with weights\n",
    "        x = x * self.weight.unsqueeze(0)\n",
    "        \n",
    "        # Apply sign function: -1 for negative and 0, 1 for positive\n",
    "        x = self.sign(x)\n",
    "        \n",
    "        # Sum the thresholded products along the input features dimension\n",
    "        x = torch.sum(x, dim=-1) \n",
    "\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "bika_linear = BiKALinear(in_features=2, out_features=3)\n",
    "input_tensor  = torch.randn(3, 2)  # Batch of 3, 10 input features each\n",
    "output_tensor = bika_linear(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d91df2d1-eae3-44cb-9150-c0176ee97367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "tensor([[[[ 0.,  1.,  2.],\n",
      "          [ 3.,  4.,  5.],\n",
      "          [ 6.,  7.,  8.]],\n",
      "\n",
      "         [[ 9., 10., 11.],\n",
      "          [12., 13., 14.],\n",
      "          [15., 16., 17.]]],\n",
      "\n",
      "\n",
      "        [[[18., 19., 20.],\n",
      "          [21., 22., 23.],\n",
      "          [24., 25., 26.]],\n",
      "\n",
      "         [[27., 28., 29.],\n",
      "          [30., 31., 32.],\n",
      "          [33., 34., 35.]]]])\n",
      "torch.Size([2, 2, 3, 3])\n",
      "\n",
      "Bias: \n",
      "Parameter containing:\n",
      "tensor([[[[-0.6859, -0.2026, -0.5440],\n",
      "          [-0.5022,  0.3358, -0.0329],\n",
      "          [-0.0755,  0.4285, -1.0047]],\n",
      "\n",
      "         [[-0.9361, -0.1238, -0.3391],\n",
      "          [ 0.0918,  0.8681,  0.7448],\n",
      "          [-0.7153, -0.4400,  0.2580]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4395,  0.7533, -0.6200],\n",
      "          [-0.4979,  0.4181, -0.1048],\n",
      "          [-1.3353, -1.1510, -0.8696]],\n",
      "\n",
      "         [[-0.7482, -1.6774,  0.0405],\n",
      "          [-1.1515, -1.1363,  0.2383],\n",
      "          [-1.1684,  0.4206, -1.0500]]],\n",
      "\n",
      "\n",
      "        [[[-0.0218,  1.5287,  0.4934],\n",
      "          [ 0.2579,  0.4149,  0.4943],\n",
      "          [-0.5038,  0.5697,  1.8474]],\n",
      "\n",
      "         [[-0.3607, -0.9326,  1.5163],\n",
      "          [ 1.3913,  1.8628,  0.6059],\n",
      "          [ 0.1643, -0.4428,  0.1451]]]], requires_grad=True)\n",
      "torch.Size([3, 2, 3, 3])\n",
      "\n",
      "Unfold Input: \n",
      "tensor([[[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  3.,  4.],\n",
      "         [ 0.,  0.,  0.,  0.,  1.,  2.,  3.,  4.,  5.],\n",
      "         [ 0.,  0.,  0.,  1.,  2.,  0.,  4.,  5.,  0.],\n",
      "         [ 0.,  0.,  1.,  0.,  3.,  4.,  0.,  6.,  7.],\n",
      "         [ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],\n",
      "         [ 1.,  2.,  0.,  4.,  5.,  0.,  7.,  8.,  0.],\n",
      "         [ 0.,  3.,  4.,  0.,  6.,  7.,  0.,  0.,  0.],\n",
      "         [ 3.,  4.,  5.,  6.,  7.,  8.,  0.,  0.,  0.],\n",
      "         [ 4.,  5.,  0.,  7.,  8.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  9., 10.,  0., 12., 13.],\n",
      "         [ 0.,  0.,  0.,  9., 10., 11., 12., 13., 14.],\n",
      "         [ 0.,  0.,  0., 10., 11.,  0., 13., 14.,  0.],\n",
      "         [ 0.,  9., 10.,  0., 12., 13.,  0., 15., 16.],\n",
      "         [ 9., 10., 11., 12., 13., 14., 15., 16., 17.],\n",
      "         [10., 11.,  0., 13., 14.,  0., 16., 17.,  0.],\n",
      "         [ 0., 12., 13.,  0., 15., 16.,  0.,  0.,  0.],\n",
      "         [12., 13., 14., 15., 16., 17.,  0.,  0.,  0.],\n",
      "         [13., 14.,  0., 16., 17.,  0.,  0.,  0.,  0.]],\n",
      "\n",
      "        [[ 0.,  0.,  0.,  0., 18., 19.,  0., 21., 22.],\n",
      "         [ 0.,  0.,  0., 18., 19., 20., 21., 22., 23.],\n",
      "         [ 0.,  0.,  0., 19., 20.,  0., 22., 23.,  0.],\n",
      "         [ 0., 18., 19.,  0., 21., 22.,  0., 24., 25.],\n",
      "         [18., 19., 20., 21., 22., 23., 24., 25., 26.],\n",
      "         [19., 20.,  0., 22., 23.,  0., 25., 26.,  0.],\n",
      "         [ 0., 21., 22.,  0., 24., 25.,  0.,  0.,  0.],\n",
      "         [21., 22., 23., 24., 25., 26.,  0.,  0.,  0.],\n",
      "         [22., 23.,  0., 25., 26.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0., 27., 28.,  0., 30., 31.],\n",
      "         [ 0.,  0.,  0., 27., 28., 29., 30., 31., 32.],\n",
      "         [ 0.,  0.,  0., 28., 29.,  0., 31., 32.,  0.],\n",
      "         [ 0., 27., 28.,  0., 30., 31.,  0., 33., 34.],\n",
      "         [27., 28., 29., 30., 31., 32., 33., 34., 35.],\n",
      "         [28., 29.,  0., 31., 32.,  0., 34., 35.,  0.],\n",
      "         [ 0., 30., 31.,  0., 33., 34.,  0.,  0.,  0.],\n",
      "         [30., 31., 32., 33., 34., 35.,  0.,  0.,  0.],\n",
      "         [31., 32.,  0., 34., 35.,  0.,  0.,  0.,  0.]]])\n",
      "torch.Size([2, 18, 9])\n",
      "\n",
      "Reshaped Unfold Input: \n",
      "tensor([[[[[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  3.,  4.],\n",
      "           [ 0.,  0.,  0.,  0.,  1.,  2.,  3.,  4.,  5.],\n",
      "           [ 0.,  0.,  0.,  1.,  2.,  0.,  4.,  5.,  0.]],\n",
      "\n",
      "          [[ 0.,  0.,  1.,  0.,  3.,  4.,  0.,  6.,  7.],\n",
      "           [ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],\n",
      "           [ 1.,  2.,  0.,  4.,  5.,  0.,  7.,  8.,  0.]],\n",
      "\n",
      "          [[ 0.,  3.,  4.,  0.,  6.,  7.,  0.,  0.,  0.],\n",
      "           [ 3.,  4.,  5.,  6.,  7.,  8.,  0.,  0.,  0.],\n",
      "           [ 4.,  5.,  0.,  7.,  8.,  0.,  0.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "         [[[ 0.,  0.,  0.,  0.,  9., 10.,  0., 12., 13.],\n",
      "           [ 0.,  0.,  0.,  9., 10., 11., 12., 13., 14.],\n",
      "           [ 0.,  0.,  0., 10., 11.,  0., 13., 14.,  0.]],\n",
      "\n",
      "          [[ 0.,  9., 10.,  0., 12., 13.,  0., 15., 16.],\n",
      "           [ 9., 10., 11., 12., 13., 14., 15., 16., 17.],\n",
      "           [10., 11.,  0., 13., 14.,  0., 16., 17.,  0.]],\n",
      "\n",
      "          [[ 0., 12., 13.,  0., 15., 16.,  0.,  0.,  0.],\n",
      "           [12., 13., 14., 15., 16., 17.,  0.,  0.,  0.],\n",
      "           [13., 14.,  0., 16., 17.,  0.,  0.,  0.,  0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.,  0.,  0.,  0., 18., 19.,  0., 21., 22.],\n",
      "           [ 0.,  0.,  0., 18., 19., 20., 21., 22., 23.],\n",
      "           [ 0.,  0.,  0., 19., 20.,  0., 22., 23.,  0.]],\n",
      "\n",
      "          [[ 0., 18., 19.,  0., 21., 22.,  0., 24., 25.],\n",
      "           [18., 19., 20., 21., 22., 23., 24., 25., 26.],\n",
      "           [19., 20.,  0., 22., 23.,  0., 25., 26.,  0.]],\n",
      "\n",
      "          [[ 0., 21., 22.,  0., 24., 25.,  0.,  0.,  0.],\n",
      "           [21., 22., 23., 24., 25., 26.,  0.,  0.,  0.],\n",
      "           [22., 23.,  0., 25., 26.,  0.,  0.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "         [[[ 0.,  0.,  0.,  0., 27., 28.,  0., 30., 31.],\n",
      "           [ 0.,  0.,  0., 27., 28., 29., 30., 31., 32.],\n",
      "           [ 0.,  0.,  0., 28., 29.,  0., 31., 32.,  0.]],\n",
      "\n",
      "          [[ 0., 27., 28.,  0., 30., 31.,  0., 33., 34.],\n",
      "           [27., 28., 29., 30., 31., 32., 33., 34., 35.],\n",
      "           [28., 29.,  0., 31., 32.,  0., 34., 35.,  0.]],\n",
      "\n",
      "          [[ 0., 30., 31.,  0., 33., 34.,  0.,  0.,  0.],\n",
      "           [30., 31., 32., 33., 34., 35.,  0.,  0.,  0.],\n",
      "           [31., 32.,  0., 34., 35.,  0.,  0.,  0.,  0.]]]]])\n",
      "torch.Size([2, 2, 3, 3, 9])\n",
      "\n",
      "Reshaped Bias: \n",
      "tensor([[[[[-0.6859],\n",
      "           [-0.2026],\n",
      "           [-0.5440]],\n",
      "\n",
      "          [[-0.5022],\n",
      "           [ 0.3358],\n",
      "           [-0.0329]],\n",
      "\n",
      "          [[-0.0755],\n",
      "           [ 0.4285],\n",
      "           [-1.0047]]],\n",
      "\n",
      "\n",
      "         [[[-0.9361],\n",
      "           [-0.1238],\n",
      "           [-0.3391]],\n",
      "\n",
      "          [[ 0.0918],\n",
      "           [ 0.8681],\n",
      "           [ 0.7448]],\n",
      "\n",
      "          [[-0.7153],\n",
      "           [-0.4400],\n",
      "           [ 0.2580]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 1.4395],\n",
      "           [ 0.7533],\n",
      "           [-0.6200]],\n",
      "\n",
      "          [[-0.4979],\n",
      "           [ 0.4181],\n",
      "           [-0.1048]],\n",
      "\n",
      "          [[-1.3353],\n",
      "           [-1.1510],\n",
      "           [-0.8696]]],\n",
      "\n",
      "\n",
      "         [[[-0.7482],\n",
      "           [-1.6774],\n",
      "           [ 0.0405]],\n",
      "\n",
      "          [[-1.1515],\n",
      "           [-1.1363],\n",
      "           [ 0.2383]],\n",
      "\n",
      "          [[-1.1684],\n",
      "           [ 0.4206],\n",
      "           [-1.0500]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-0.0218],\n",
      "           [ 1.5287],\n",
      "           [ 0.4934]],\n",
      "\n",
      "          [[ 0.2579],\n",
      "           [ 0.4149],\n",
      "           [ 0.4943]],\n",
      "\n",
      "          [[-0.5038],\n",
      "           [ 0.5697],\n",
      "           [ 1.8474]]],\n",
      "\n",
      "\n",
      "         [[[-0.3607],\n",
      "           [-0.9326],\n",
      "           [ 1.5163]],\n",
      "\n",
      "          [[ 1.3913],\n",
      "           [ 1.8628],\n",
      "           [ 0.6059]],\n",
      "\n",
      "          [[ 0.1643],\n",
      "           [-0.4428],\n",
      "           [ 0.1451]]]]], grad_fn=<UnsqueezeBackward0>)\n",
      "torch.Size([3, 2, 3, 3, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m bika_conv \u001b[38;5;241m=\u001b[39m BiKAConv2D(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     79\u001b[0m input_tensor  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m36\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mbika_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 48\u001b[0m, in \u001b[0;36mBiKAConv2D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 48\u001b[0m modified_input \u001b[38;5;241m=\u001b[39m \u001b[43mmodified_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# modified_input = unfolded_input + self.bias.unsqueeze(0).unsqueeze(-1)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# modified_input = modified_input + self.bias.unsqueeze(0).unsqueeze(-1)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBiased Input: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "class BiKAConv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        super(BiKAConv2D, self).__init__()\n",
    "        # Define weights for convolution\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
    "        )\n",
    "        # Define an individual bias for each weight in the kernel\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
    "        )\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "\n",
    "    def forward(self, x):        \n",
    "        print(\"\\nInput: \")\n",
    "        print(x)\n",
    "        print(x.shape)\n",
    "        print(\"\\nBias: \")\n",
    "        print(self.bias)\n",
    "        print(self.bias.shape)\n",
    "        \n",
    "        # Add the bias to each activation before multiplying by the weight\n",
    "        # Equivalent to computing w * (a + b) for each kernel position\n",
    "        modified_input = F.unfold(x, kernel_size=self.weight.shape[2:], stride=self.stride, padding=self.padding)\n",
    "        # batch_size, in_channels, height, width = x.shape\n",
    "        # kernel_height, kernel_width = self.weight.shape[2:]\n",
    "        # modified_input = F.unfold(x, kernel_size=(kernel_height, kernel_width), stride=self.stride, padding=self.padding)\n",
    "        # unfolded_input = F.unfold(x, kernel_size=(kernel_height, kernel_width), stride=self.stride, padding=self.padding)\n",
    "        print(\"\\nUnfold Input: \")\n",
    "        print(modified_input)\n",
    "        print(modified_input.shape)\n",
    "        \n",
    "        modified_input = modified_input.view(\n",
    "             x.shape[0], x.shape[1], self.weight.shape[2], self.weight.shape[3], -1\n",
    "        )\n",
    "        # unfolded_input = unfolded_input.view(batch_size, in_channels, kernel_height, kernel_width, -1)\n",
    "        # modified_input = modified_input.view(batch_size, in_channels, kernel_height, kernel_width, -1)\n",
    "        print(\"\\nReshaped Unfold Input: \")\n",
    "        print(modified_input)\n",
    "        print(modified_input.shape)\n",
    "        \n",
    "        print(\"\\nReshaped Bias: \")\n",
    "        print(self.bias.unsqueeze(-1))\n",
    "        print(self.bias.unsqueeze(-1).shape)\n",
    "        modified_input = modified_input + self.bias.unsqueeze(-1)\n",
    "        # modified_input = unfolded_input + self.bias.unsqueeze(0).unsqueeze(-1)\n",
    "        # modified_input = modified_input + self.bias.unsqueeze(0).unsqueeze(-1)\n",
    "        print(\"\\nBiased Input: \")\n",
    "        print(modified_input)\n",
    "        print(modified_input.shape)\n",
    "        \n",
    "        modified_input = modified_input.view(x.shape[0], -1, modified_input.shape[-1])\n",
    "        print(modified_input)\n",
    "        print(modified_input.shape)\n",
    "        \n",
    "        print(self.weight)\n",
    "        print(self.weight.shape)\n",
    "        # Perform the convolution with the modified input\n",
    "        weight_reshaped = self.weight.view(self.weight.shape[0], -1)\n",
    "        print(weight_reshaped)\n",
    "        print(weight_reshaped.shape)\n",
    "        print(weight_reshaped.shape[0])\n",
    "        print(weight_reshaped.shape[1])\n",
    "        print(weight_reshaped.shape[1])\n",
    "        print(weight_reshaped.reshape(1, weight_reshaped.shape[0]*weight_reshaped.shape[1]).unsqueeze(-1).repeat(1, 1, 2))\n",
    "        print(weight_reshaped.reshape(1, weight_reshaped.shape[0]*weight_reshaped.shape[1]).unsqueeze(-1).repeat(1, 1, 2).shape)\n",
    "        print(weight_reshaped.unsqueeze(0).expand(x.shape[0], -1, -1))\n",
    "        print(weight_reshaped.unsqueeze(0).expand(x.shape[0], -1, -1).shape)\n",
    "        output = torch.bmm(weight_reshaped.unsqueeze(0).expand(x.shape[0], -1, -1), modified_input)\n",
    "        output = output.view(x.shape[0], self.weight.shape[0], int(x.shape[2]/self.stride), int(x.shape[3]/self.stride))\n",
    "\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "bika_conv = BiKAConv2D(in_channels=2, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "input_tensor  = torch.arange(0, 36, dtype=torch.float32).reshape(2,2,3,3)\n",
    "output_tensor = bika_conv(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a278449-8168-44ac-a20d-ded07c23b21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.,  1.,  2.,  3.],\n",
      "          [ 4.,  5.,  6.,  7.],\n",
      "          [ 8.,  9., 10., 11.],\n",
      "          [12., 13., 14., 15.]]]])\n",
      "torch.Size([1, 1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor)\n",
    "print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc688e0-b2e8-4f15-8ffa-3e826e902de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_tensor)\n",
    "print(output_tensor.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
