{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cfd9c06-5604-49a2-9fdc-4217b483a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d21be32-53be-49ff-99af-d7a5388e2afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output during inference: tensor([ 1., -1., -1.,  1.], grad_fn=<CustomSignFunctionBackward>)\n",
      "Gradient during training: tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "class CustomSignFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Save the input for backward computation\n",
    "        ctx.save_for_backward(input)\n",
    "        # Output +1 for input > 0, else -1 (including for input == 0)\n",
    "        return torch.where(input > 0, torch.tensor(1.0, device=input.device), torch.tensor(-1.0, device=input.device))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve the input saved in the forward pass\n",
    "        input, = ctx.saved_tensors\n",
    "        # Gradient of the input is the same as the gradient output (STE)\n",
    "        grad_input = grad_output.clone()\n",
    "        # Pass the gradient only where input was non-zero, otherwise set it to 0\n",
    "        grad_input[input.abs() > 0] = grad_output[input.abs() > 0]\n",
    "        return grad_input\n",
    "\n",
    "# Wrapper class for convenience\n",
    "class CustomSignActivation(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomSignActivation, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return CustomSignFunction.apply(input)\n",
    "\n",
    "# Example usage:\n",
    "sign_activation = CustomSignActivation()\n",
    "\n",
    "# Test the forward pass\n",
    "x = torch.tensor([2.0, -3.0, 0.0, 1.5], requires_grad=True)\n",
    "output = sign_activation(x)\n",
    "print(\"Output during inference:\", output)\n",
    "\n",
    "# Test the backward pass (gradient computation during training)\n",
    "loss = output.sum()  # Just an example loss\n",
    "loss.backward()\n",
    "print(\"Gradient during training:\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d91df2d1-eae3-44cb-9150-c0176ee97367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.,  1.,  2.,  3.],\n",
      "          [ 4.,  5.,  6.,  7.],\n",
      "          [ 8.,  9., 10., 11.],\n",
      "          [12., 13., 14., 15.]]]])\n",
      "torch.Size([1, 1, 4, 4])\n",
      "Parameter containing:\n",
      "tensor([[[[-1.5539, -0.3945, -1.5523],\n",
      "          [ 0.7011,  1.9568, -0.7478],\n",
      "          [-1.7810, -1.9629,  1.2301]]],\n",
      "\n",
      "\n",
      "        [[[-1.3081,  0.2781, -0.0061],\n",
      "          [-1.0853,  0.4958, -1.7125],\n",
      "          [ 1.5011, -0.1729, -0.1830]]],\n",
      "\n",
      "\n",
      "        [[[-1.0890, -0.7653,  0.0072],\n",
      "          [ 0.5572, -1.3871, -0.9199],\n",
      "          [ 0.0359, -1.1856, -2.6031]]]], requires_grad=True)\n",
      "torch.Size([3, 1, 3, 3])\n",
      "tensor([[[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  2.,  0.,  4.,  5.,  6.,  0.,  8.,\n",
      "           9., 10.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.,\n",
      "          10., 11.],\n",
      "         [ 0.,  0.,  0.,  0.,  1.,  2.,  3.,  0.,  5.,  6.,  7.,  0.,  9., 10.,\n",
      "          11.,  0.],\n",
      "         [ 0.,  0.,  1.,  2.,  0.,  4.,  5.,  6.,  0.,  8.,  9., 10.,  0., 12.,\n",
      "          13., 14.],\n",
      "         [ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "          14., 15.],\n",
      "         [ 1.,  2.,  3.,  0.,  5.,  6.,  7.,  0.,  9., 10., 11.,  0., 13., 14.,\n",
      "          15.,  0.],\n",
      "         [ 0.,  4.,  5.,  6.,  0.,  8.,  9., 10.,  0., 12., 13., 14.,  0.,  0.,\n",
      "           0.,  0.],\n",
      "         [ 4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15.,  0.,  0.,\n",
      "           0.,  0.],\n",
      "         [ 5.,  6.,  7.,  0.,  9., 10., 11.,  0., 13., 14., 15.,  0.,  0.,  0.,\n",
      "           0.,  0.]]])\n",
      "torch.Size([1, 9, 16])\n",
      "tensor([[[[[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  2.,  0.,  4.,  5.,  6.,  0.,\n",
      "             8.,  9., 10.],\n",
      "           [ 0.,  0.,  0.,  0.,  0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,\n",
      "             9., 10., 11.],\n",
      "           [ 0.,  0.,  0.,  0.,  1.,  2.,  3.,  0.,  5.,  6.,  7.,  0.,  9.,\n",
      "            10., 11.,  0.]],\n",
      "\n",
      "          [[ 0.,  0.,  1.,  2.,  0.,  4.,  5.,  6.,  0.,  8.,  9., 10.,  0.,\n",
      "            12., 13., 14.],\n",
      "           [ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n",
      "            13., 14., 15.],\n",
      "           [ 1.,  2.,  3.,  0.,  5.,  6.,  7.,  0.,  9., 10., 11.,  0., 13.,\n",
      "            14., 15.,  0.]],\n",
      "\n",
      "          [[ 0.,  4.,  5.,  6.,  0.,  8.,  9., 10.,  0., 12., 13., 14.,  0.,\n",
      "             0.,  0.,  0.],\n",
      "           [ 4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15.,  0.,\n",
      "             0.,  0.,  0.],\n",
      "           [ 5.,  6.,  7.,  0.,  9., 10., 11.,  0., 13., 14., 15.,  0.,  0.,\n",
      "             0.,  0.,  0.]]]]])\n",
      "torch.Size([1, 1, 3, 3, 16])\n",
      "tensor([[[[[-1.5539e+00, -1.5539e+00, -1.5539e+00, -1.5539e+00, -1.5539e+00,\n",
      "            -1.5539e+00, -5.5385e-01,  4.4615e-01, -1.5539e+00,  2.4461e+00,\n",
      "             3.4461e+00,  4.4461e+00, -1.5539e+00,  6.4461e+00,  7.4461e+00,\n",
      "             8.4461e+00],\n",
      "           [-3.9450e-01, -3.9450e-01, -3.9450e-01, -3.9450e-01, -3.9450e-01,\n",
      "             6.0550e-01,  1.6055e+00,  2.6055e+00,  3.6055e+00,  4.6055e+00,\n",
      "             5.6055e+00,  6.6055e+00,  7.6055e+00,  8.6055e+00,  9.6055e+00,\n",
      "             1.0606e+01],\n",
      "           [-1.5523e+00, -1.5523e+00, -1.5523e+00, -1.5523e+00, -5.5232e-01,\n",
      "             4.4768e-01,  1.4477e+00, -1.5523e+00,  3.4477e+00,  4.4477e+00,\n",
      "             5.4477e+00, -1.5523e+00,  7.4477e+00,  8.4477e+00,  9.4477e+00,\n",
      "            -1.5523e+00]],\n",
      "\n",
      "          [[ 7.0108e-01,  7.0108e-01,  1.7011e+00,  2.7011e+00,  7.0108e-01,\n",
      "             4.7011e+00,  5.7011e+00,  6.7011e+00,  7.0108e-01,  8.7011e+00,\n",
      "             9.7011e+00,  1.0701e+01,  7.0108e-01,  1.2701e+01,  1.3701e+01,\n",
      "             1.4701e+01],\n",
      "           [ 1.9568e+00,  2.9568e+00,  3.9568e+00,  4.9568e+00,  5.9568e+00,\n",
      "             6.9568e+00,  7.9568e+00,  8.9568e+00,  9.9568e+00,  1.0957e+01,\n",
      "             1.1957e+01,  1.2957e+01,  1.3957e+01,  1.4957e+01,  1.5957e+01,\n",
      "             1.6957e+01],\n",
      "           [ 2.5223e-01,  1.2522e+00,  2.2522e+00, -7.4777e-01,  4.2522e+00,\n",
      "             5.2522e+00,  6.2522e+00, -7.4777e-01,  8.2522e+00,  9.2522e+00,\n",
      "             1.0252e+01, -7.4777e-01,  1.2252e+01,  1.3252e+01,  1.4252e+01,\n",
      "            -7.4777e-01]],\n",
      "\n",
      "          [[-1.7810e+00,  2.2190e+00,  3.2190e+00,  4.2190e+00, -1.7810e+00,\n",
      "             6.2190e+00,  7.2190e+00,  8.2190e+00, -1.7810e+00,  1.0219e+01,\n",
      "             1.1219e+01,  1.2219e+01, -1.7810e+00, -1.7810e+00, -1.7810e+00,\n",
      "            -1.7810e+00],\n",
      "           [ 2.0371e+00,  3.0371e+00,  4.0371e+00,  5.0371e+00,  6.0371e+00,\n",
      "             7.0371e+00,  8.0371e+00,  9.0371e+00,  1.0037e+01,  1.1037e+01,\n",
      "             1.2037e+01,  1.3037e+01, -1.9629e+00, -1.9629e+00, -1.9629e+00,\n",
      "            -1.9629e+00],\n",
      "           [ 6.2301e+00,  7.2301e+00,  8.2301e+00,  1.2301e+00,  1.0230e+01,\n",
      "             1.1230e+01,  1.2230e+01,  1.2301e+00,  1.4230e+01,  1.5230e+01,\n",
      "             1.6230e+01,  1.2301e+00,  1.2301e+00,  1.2301e+00,  1.2301e+00,\n",
      "             1.2301e+00]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-1.3081e+00, -1.3081e+00, -1.3081e+00, -1.3081e+00, -1.3081e+00,\n",
      "            -1.3081e+00, -3.0806e-01,  6.9194e-01, -1.3081e+00,  2.6919e+00,\n",
      "             3.6919e+00,  4.6919e+00, -1.3081e+00,  6.6919e+00,  7.6919e+00,\n",
      "             8.6919e+00],\n",
      "           [ 2.7809e-01,  2.7809e-01,  2.7809e-01,  2.7809e-01,  2.7809e-01,\n",
      "             1.2781e+00,  2.2781e+00,  3.2781e+00,  4.2781e+00,  5.2781e+00,\n",
      "             6.2781e+00,  7.2781e+00,  8.2781e+00,  9.2781e+00,  1.0278e+01,\n",
      "             1.1278e+01],\n",
      "           [-6.0926e-03, -6.0926e-03, -6.0926e-03, -6.0926e-03,  9.9391e-01,\n",
      "             1.9939e+00,  2.9939e+00, -6.0926e-03,  4.9939e+00,  5.9939e+00,\n",
      "             6.9939e+00, -6.0926e-03,  8.9939e+00,  9.9939e+00,  1.0994e+01,\n",
      "            -6.0926e-03]],\n",
      "\n",
      "          [[-1.0853e+00, -1.0853e+00, -8.5270e-02,  9.1473e-01, -1.0853e+00,\n",
      "             2.9147e+00,  3.9147e+00,  4.9147e+00, -1.0853e+00,  6.9147e+00,\n",
      "             7.9147e+00,  8.9147e+00, -1.0853e+00,  1.0915e+01,  1.1915e+01,\n",
      "             1.2915e+01],\n",
      "           [ 4.9577e-01,  1.4958e+00,  2.4958e+00,  3.4958e+00,  4.4958e+00,\n",
      "             5.4958e+00,  6.4958e+00,  7.4958e+00,  8.4958e+00,  9.4958e+00,\n",
      "             1.0496e+01,  1.1496e+01,  1.2496e+01,  1.3496e+01,  1.4496e+01,\n",
      "             1.5496e+01],\n",
      "           [-7.1246e-01,  2.8754e-01,  1.2875e+00, -1.7125e+00,  3.2875e+00,\n",
      "             4.2875e+00,  5.2875e+00, -1.7125e+00,  7.2875e+00,  8.2875e+00,\n",
      "             9.2875e+00, -1.7125e+00,  1.1288e+01,  1.2288e+01,  1.3288e+01,\n",
      "            -1.7125e+00]],\n",
      "\n",
      "          [[ 1.5011e+00,  5.5011e+00,  6.5011e+00,  7.5011e+00,  1.5011e+00,\n",
      "             9.5011e+00,  1.0501e+01,  1.1501e+01,  1.5011e+00,  1.3501e+01,\n",
      "             1.4501e+01,  1.5501e+01,  1.5011e+00,  1.5011e+00,  1.5011e+00,\n",
      "             1.5011e+00],\n",
      "           [ 3.8271e+00,  4.8271e+00,  5.8271e+00,  6.8271e+00,  7.8271e+00,\n",
      "             8.8271e+00,  9.8271e+00,  1.0827e+01,  1.1827e+01,  1.2827e+01,\n",
      "             1.3827e+01,  1.4827e+01, -1.7291e-01, -1.7291e-01, -1.7291e-01,\n",
      "            -1.7291e-01],\n",
      "           [ 4.8170e+00,  5.8170e+00,  6.8170e+00, -1.8301e-01,  8.8170e+00,\n",
      "             9.8170e+00,  1.0817e+01, -1.8301e-01,  1.2817e+01,  1.3817e+01,\n",
      "             1.4817e+01, -1.8301e-01, -1.8301e-01, -1.8301e-01, -1.8301e-01,\n",
      "            -1.8301e-01]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-1.0890e+00, -1.0890e+00, -1.0890e+00, -1.0890e+00, -1.0890e+00,\n",
      "            -1.0890e+00, -8.9033e-02,  9.1097e-01, -1.0890e+00,  2.9110e+00,\n",
      "             3.9110e+00,  4.9110e+00, -1.0890e+00,  6.9110e+00,  7.9110e+00,\n",
      "             8.9110e+00],\n",
      "           [-7.6531e-01, -7.6531e-01, -7.6531e-01, -7.6531e-01, -7.6531e-01,\n",
      "             2.3469e-01,  1.2347e+00,  2.2347e+00,  3.2347e+00,  4.2347e+00,\n",
      "             5.2347e+00,  6.2347e+00,  7.2347e+00,  8.2347e+00,  9.2347e+00,\n",
      "             1.0235e+01],\n",
      "           [ 7.2209e-03,  7.2209e-03,  7.2209e-03,  7.2209e-03,  1.0072e+00,\n",
      "             2.0072e+00,  3.0072e+00,  7.2209e-03,  5.0072e+00,  6.0072e+00,\n",
      "             7.0072e+00,  7.2209e-03,  9.0072e+00,  1.0007e+01,  1.1007e+01,\n",
      "             7.2209e-03]],\n",
      "\n",
      "          [[ 5.5720e-01,  5.5720e-01,  1.5572e+00,  2.5572e+00,  5.5720e-01,\n",
      "             4.5572e+00,  5.5572e+00,  6.5572e+00,  5.5720e-01,  8.5572e+00,\n",
      "             9.5572e+00,  1.0557e+01,  5.5720e-01,  1.2557e+01,  1.3557e+01,\n",
      "             1.4557e+01],\n",
      "           [-1.3871e+00, -3.8710e-01,  6.1290e-01,  1.6129e+00,  2.6129e+00,\n",
      "             3.6129e+00,  4.6129e+00,  5.6129e+00,  6.6129e+00,  7.6129e+00,\n",
      "             8.6129e+00,  9.6129e+00,  1.0613e+01,  1.1613e+01,  1.2613e+01,\n",
      "             1.3613e+01],\n",
      "           [ 8.0097e-02,  1.0801e+00,  2.0801e+00, -9.1990e-01,  4.0801e+00,\n",
      "             5.0801e+00,  6.0801e+00, -9.1990e-01,  8.0801e+00,  9.0801e+00,\n",
      "             1.0080e+01, -9.1990e-01,  1.2080e+01,  1.3080e+01,  1.4080e+01,\n",
      "            -9.1990e-01]],\n",
      "\n",
      "          [[ 3.5920e-02,  4.0359e+00,  5.0359e+00,  6.0359e+00,  3.5920e-02,\n",
      "             8.0359e+00,  9.0359e+00,  1.0036e+01,  3.5920e-02,  1.2036e+01,\n",
      "             1.3036e+01,  1.4036e+01,  3.5920e-02,  3.5920e-02,  3.5920e-02,\n",
      "             3.5920e-02],\n",
      "           [ 2.8144e+00,  3.8144e+00,  4.8144e+00,  5.8144e+00,  6.8144e+00,\n",
      "             7.8144e+00,  8.8144e+00,  9.8144e+00,  1.0814e+01,  1.1814e+01,\n",
      "             1.2814e+01,  1.3814e+01, -1.1856e+00, -1.1856e+00, -1.1856e+00,\n",
      "            -1.1856e+00],\n",
      "           [ 2.3969e+00,  3.3969e+00,  4.3969e+00, -2.6031e+00,  6.3969e+00,\n",
      "             7.3969e+00,  8.3969e+00, -2.6031e+00,  1.0397e+01,  1.1397e+01,\n",
      "             1.2397e+01, -2.6031e+00, -2.6031e+00, -2.6031e+00, -2.6031e+00,\n",
      "            -2.6031e+00]]]]], grad_fn=<AddBackward0>)\n",
      "torch.Size([3, 1, 3, 3, 16])\n",
      "tensor([[[-1.5539e+00, -1.5539e+00, -1.5539e+00, -1.5539e+00, -1.5539e+00,\n",
      "          -1.5539e+00, -5.5385e-01,  4.4615e-01, -1.5539e+00,  2.4461e+00,\n",
      "           3.4461e+00,  4.4461e+00, -1.5539e+00,  6.4461e+00,  7.4461e+00,\n",
      "           8.4461e+00],\n",
      "         [-3.9450e-01, -3.9450e-01, -3.9450e-01, -3.9450e-01, -3.9450e-01,\n",
      "           6.0550e-01,  1.6055e+00,  2.6055e+00,  3.6055e+00,  4.6055e+00,\n",
      "           5.6055e+00,  6.6055e+00,  7.6055e+00,  8.6055e+00,  9.6055e+00,\n",
      "           1.0606e+01],\n",
      "         [-1.5523e+00, -1.5523e+00, -1.5523e+00, -1.5523e+00, -5.5232e-01,\n",
      "           4.4768e-01,  1.4477e+00, -1.5523e+00,  3.4477e+00,  4.4477e+00,\n",
      "           5.4477e+00, -1.5523e+00,  7.4477e+00,  8.4477e+00,  9.4477e+00,\n",
      "          -1.5523e+00],\n",
      "         [ 7.0108e-01,  7.0108e-01,  1.7011e+00,  2.7011e+00,  7.0108e-01,\n",
      "           4.7011e+00,  5.7011e+00,  6.7011e+00,  7.0108e-01,  8.7011e+00,\n",
      "           9.7011e+00,  1.0701e+01,  7.0108e-01,  1.2701e+01,  1.3701e+01,\n",
      "           1.4701e+01],\n",
      "         [ 1.9568e+00,  2.9568e+00,  3.9568e+00,  4.9568e+00,  5.9568e+00,\n",
      "           6.9568e+00,  7.9568e+00,  8.9568e+00,  9.9568e+00,  1.0957e+01,\n",
      "           1.1957e+01,  1.2957e+01,  1.3957e+01,  1.4957e+01,  1.5957e+01,\n",
      "           1.6957e+01],\n",
      "         [ 2.5223e-01,  1.2522e+00,  2.2522e+00, -7.4777e-01,  4.2522e+00,\n",
      "           5.2522e+00,  6.2522e+00, -7.4777e-01,  8.2522e+00,  9.2522e+00,\n",
      "           1.0252e+01, -7.4777e-01,  1.2252e+01,  1.3252e+01,  1.4252e+01,\n",
      "          -7.4777e-01],\n",
      "         [-1.7810e+00,  2.2190e+00,  3.2190e+00,  4.2190e+00, -1.7810e+00,\n",
      "           6.2190e+00,  7.2190e+00,  8.2190e+00, -1.7810e+00,  1.0219e+01,\n",
      "           1.1219e+01,  1.2219e+01, -1.7810e+00, -1.7810e+00, -1.7810e+00,\n",
      "          -1.7810e+00],\n",
      "         [ 2.0371e+00,  3.0371e+00,  4.0371e+00,  5.0371e+00,  6.0371e+00,\n",
      "           7.0371e+00,  8.0371e+00,  9.0371e+00,  1.0037e+01,  1.1037e+01,\n",
      "           1.2037e+01,  1.3037e+01, -1.9629e+00, -1.9629e+00, -1.9629e+00,\n",
      "          -1.9629e+00],\n",
      "         [ 6.2301e+00,  7.2301e+00,  8.2301e+00,  1.2301e+00,  1.0230e+01,\n",
      "           1.1230e+01,  1.2230e+01,  1.2301e+00,  1.4230e+01,  1.5230e+01,\n",
      "           1.6230e+01,  1.2301e+00,  1.2301e+00,  1.2301e+00,  1.2301e+00,\n",
      "           1.2301e+00],\n",
      "         [-1.3081e+00, -1.3081e+00, -1.3081e+00, -1.3081e+00, -1.3081e+00,\n",
      "          -1.3081e+00, -3.0806e-01,  6.9194e-01, -1.3081e+00,  2.6919e+00,\n",
      "           3.6919e+00,  4.6919e+00, -1.3081e+00,  6.6919e+00,  7.6919e+00,\n",
      "           8.6919e+00],\n",
      "         [ 2.7809e-01,  2.7809e-01,  2.7809e-01,  2.7809e-01,  2.7809e-01,\n",
      "           1.2781e+00,  2.2781e+00,  3.2781e+00,  4.2781e+00,  5.2781e+00,\n",
      "           6.2781e+00,  7.2781e+00,  8.2781e+00,  9.2781e+00,  1.0278e+01,\n",
      "           1.1278e+01],\n",
      "         [-6.0926e-03, -6.0926e-03, -6.0926e-03, -6.0926e-03,  9.9391e-01,\n",
      "           1.9939e+00,  2.9939e+00, -6.0926e-03,  4.9939e+00,  5.9939e+00,\n",
      "           6.9939e+00, -6.0926e-03,  8.9939e+00,  9.9939e+00,  1.0994e+01,\n",
      "          -6.0926e-03],\n",
      "         [-1.0853e+00, -1.0853e+00, -8.5270e-02,  9.1473e-01, -1.0853e+00,\n",
      "           2.9147e+00,  3.9147e+00,  4.9147e+00, -1.0853e+00,  6.9147e+00,\n",
      "           7.9147e+00,  8.9147e+00, -1.0853e+00,  1.0915e+01,  1.1915e+01,\n",
      "           1.2915e+01],\n",
      "         [ 4.9577e-01,  1.4958e+00,  2.4958e+00,  3.4958e+00,  4.4958e+00,\n",
      "           5.4958e+00,  6.4958e+00,  7.4958e+00,  8.4958e+00,  9.4958e+00,\n",
      "           1.0496e+01,  1.1496e+01,  1.2496e+01,  1.3496e+01,  1.4496e+01,\n",
      "           1.5496e+01],\n",
      "         [-7.1246e-01,  2.8754e-01,  1.2875e+00, -1.7125e+00,  3.2875e+00,\n",
      "           4.2875e+00,  5.2875e+00, -1.7125e+00,  7.2875e+00,  8.2875e+00,\n",
      "           9.2875e+00, -1.7125e+00,  1.1288e+01,  1.2288e+01,  1.3288e+01,\n",
      "          -1.7125e+00],\n",
      "         [ 1.5011e+00,  5.5011e+00,  6.5011e+00,  7.5011e+00,  1.5011e+00,\n",
      "           9.5011e+00,  1.0501e+01,  1.1501e+01,  1.5011e+00,  1.3501e+01,\n",
      "           1.4501e+01,  1.5501e+01,  1.5011e+00,  1.5011e+00,  1.5011e+00,\n",
      "           1.5011e+00],\n",
      "         [ 3.8271e+00,  4.8271e+00,  5.8271e+00,  6.8271e+00,  7.8271e+00,\n",
      "           8.8271e+00,  9.8271e+00,  1.0827e+01,  1.1827e+01,  1.2827e+01,\n",
      "           1.3827e+01,  1.4827e+01, -1.7291e-01, -1.7291e-01, -1.7291e-01,\n",
      "          -1.7291e-01],\n",
      "         [ 4.8170e+00,  5.8170e+00,  6.8170e+00, -1.8301e-01,  8.8170e+00,\n",
      "           9.8170e+00,  1.0817e+01, -1.8301e-01,  1.2817e+01,  1.3817e+01,\n",
      "           1.4817e+01, -1.8301e-01, -1.8301e-01, -1.8301e-01, -1.8301e-01,\n",
      "          -1.8301e-01],\n",
      "         [-1.0890e+00, -1.0890e+00, -1.0890e+00, -1.0890e+00, -1.0890e+00,\n",
      "          -1.0890e+00, -8.9033e-02,  9.1097e-01, -1.0890e+00,  2.9110e+00,\n",
      "           3.9110e+00,  4.9110e+00, -1.0890e+00,  6.9110e+00,  7.9110e+00,\n",
      "           8.9110e+00],\n",
      "         [-7.6531e-01, -7.6531e-01, -7.6531e-01, -7.6531e-01, -7.6531e-01,\n",
      "           2.3469e-01,  1.2347e+00,  2.2347e+00,  3.2347e+00,  4.2347e+00,\n",
      "           5.2347e+00,  6.2347e+00,  7.2347e+00,  8.2347e+00,  9.2347e+00,\n",
      "           1.0235e+01],\n",
      "         [ 7.2209e-03,  7.2209e-03,  7.2209e-03,  7.2209e-03,  1.0072e+00,\n",
      "           2.0072e+00,  3.0072e+00,  7.2209e-03,  5.0072e+00,  6.0072e+00,\n",
      "           7.0072e+00,  7.2209e-03,  9.0072e+00,  1.0007e+01,  1.1007e+01,\n",
      "           7.2209e-03],\n",
      "         [ 5.5720e-01,  5.5720e-01,  1.5572e+00,  2.5572e+00,  5.5720e-01,\n",
      "           4.5572e+00,  5.5572e+00,  6.5572e+00,  5.5720e-01,  8.5572e+00,\n",
      "           9.5572e+00,  1.0557e+01,  5.5720e-01,  1.2557e+01,  1.3557e+01,\n",
      "           1.4557e+01],\n",
      "         [-1.3871e+00, -3.8710e-01,  6.1290e-01,  1.6129e+00,  2.6129e+00,\n",
      "           3.6129e+00,  4.6129e+00,  5.6129e+00,  6.6129e+00,  7.6129e+00,\n",
      "           8.6129e+00,  9.6129e+00,  1.0613e+01,  1.1613e+01,  1.2613e+01,\n",
      "           1.3613e+01],\n",
      "         [ 8.0097e-02,  1.0801e+00,  2.0801e+00, -9.1990e-01,  4.0801e+00,\n",
      "           5.0801e+00,  6.0801e+00, -9.1990e-01,  8.0801e+00,  9.0801e+00,\n",
      "           1.0080e+01, -9.1990e-01,  1.2080e+01,  1.3080e+01,  1.4080e+01,\n",
      "          -9.1990e-01],\n",
      "         [ 3.5920e-02,  4.0359e+00,  5.0359e+00,  6.0359e+00,  3.5920e-02,\n",
      "           8.0359e+00,  9.0359e+00,  1.0036e+01,  3.5920e-02,  1.2036e+01,\n",
      "           1.3036e+01,  1.4036e+01,  3.5920e-02,  3.5920e-02,  3.5920e-02,\n",
      "           3.5920e-02],\n",
      "         [ 2.8144e+00,  3.8144e+00,  4.8144e+00,  5.8144e+00,  6.8144e+00,\n",
      "           7.8144e+00,  8.8144e+00,  9.8144e+00,  1.0814e+01,  1.1814e+01,\n",
      "           1.2814e+01,  1.3814e+01, -1.1856e+00, -1.1856e+00, -1.1856e+00,\n",
      "          -1.1856e+00],\n",
      "         [ 2.3969e+00,  3.3969e+00,  4.3969e+00, -2.6031e+00,  6.3969e+00,\n",
      "           7.3969e+00,  8.3969e+00, -2.6031e+00,  1.0397e+01,  1.1397e+01,\n",
      "           1.2397e+01, -2.6031e+00, -2.6031e+00, -2.6031e+00, -2.6031e+00,\n",
      "          -2.6031e+00]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 27, 16])\n",
      "Parameter containing:\n",
      "tensor([[[[ 1.0869, -0.2737,  0.5859],\n",
      "          [-0.5541,  0.3013,  0.8441],\n",
      "          [ 0.0710,  0.2028,  0.6020]]],\n",
      "\n",
      "\n",
      "        [[[-1.0008, -1.7788, -0.9170],\n",
      "          [ 0.7412,  0.9043, -0.0654],\n",
      "          [-0.8470,  0.0458, -0.4032]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6524, -0.0498,  1.2410],\n",
      "          [ 0.5409,  0.2914,  0.6856],\n",
      "          [ 0.0853,  0.1204, -2.1705]]]], requires_grad=True)\n",
      "torch.Size([3, 1, 3, 3])\n",
      "tensor([[ 1.0869, -0.2737,  0.5859, -0.5541,  0.3013,  0.8441,  0.0710,  0.2028,\n",
      "          0.6020],\n",
      "        [-1.0008, -1.7788, -0.9170,  0.7412,  0.9043, -0.0654, -0.8470,  0.0458,\n",
      "         -0.4032],\n",
      "        [ 0.6524, -0.0498,  1.2410,  0.5409,  0.2914,  0.6856,  0.0853,  0.1204,\n",
      "         -2.1705]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([3, 9])\n",
      "3\n",
      "9\n",
      "9\n",
      "tensor([[[ 1.0869,  1.0869],\n",
      "         [-0.2737, -0.2737],\n",
      "         [ 0.5859,  0.5859],\n",
      "         [-0.5541, -0.5541],\n",
      "         [ 0.3013,  0.3013],\n",
      "         [ 0.8441,  0.8441],\n",
      "         [ 0.0710,  0.0710],\n",
      "         [ 0.2028,  0.2028],\n",
      "         [ 0.6020,  0.6020],\n",
      "         [-1.0008, -1.0008],\n",
      "         [-1.7788, -1.7788],\n",
      "         [-0.9170, -0.9170],\n",
      "         [ 0.7412,  0.7412],\n",
      "         [ 0.9043,  0.9043],\n",
      "         [-0.0654, -0.0654],\n",
      "         [-0.8470, -0.8470],\n",
      "         [ 0.0458,  0.0458],\n",
      "         [-0.4032, -0.4032],\n",
      "         [ 0.6524,  0.6524],\n",
      "         [-0.0498, -0.0498],\n",
      "         [ 1.2410,  1.2410],\n",
      "         [ 0.5409,  0.5409],\n",
      "         [ 0.2914,  0.2914],\n",
      "         [ 0.6856,  0.6856],\n",
      "         [ 0.0853,  0.0853],\n",
      "         [ 0.1204,  0.1204],\n",
      "         [-2.1705, -2.1705]]], grad_fn=<RepeatBackward0>)\n",
      "torch.Size([1, 27, 2])\n",
      "tensor([[[ 1.0869, -0.2737,  0.5859, -0.5541,  0.3013,  0.8441,  0.0710,\n",
      "           0.2028,  0.6020],\n",
      "         [-1.0008, -1.7788, -0.9170,  0.7412,  0.9043, -0.0654, -0.8470,\n",
      "           0.0458, -0.4032],\n",
      "         [ 0.6524, -0.0498,  1.2410,  0.5409,  0.2914,  0.6856,  0.0853,\n",
      "           0.1204, -2.1705]]], grad_fn=<ExpandBackward0>)\n",
      "torch.Size([1, 3, 9])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [1, 9] but got: [1, 27].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m bika_conv \u001b[38;5;241m=\u001b[39m BiKAConv2D(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     59\u001b[0m input_tensor  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m16\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mbika_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Software/Conda_Envs/KAN/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Software/Conda_Envs/KAN/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 52\u001b[0m, in \u001b[0;36mBiKAConv2D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(weight_reshaped\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(weight_reshaped\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 52\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_reshaped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodified_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride), \u001b[38;5;28mint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride))\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [1, 9] but got: [1, 27]."
     ]
    }
   ],
   "source": [
    "class BiKAConv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        super(BiKAConv2D, self).__init__()\n",
    "        # Define weights for convolution\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
    "        )\n",
    "        # Define an individual bias for each weight in the kernel\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
    "        )\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x)\n",
    "        print(x.shape)\n",
    "        print(self.bias)\n",
    "        print(self.bias.shape)\n",
    "        # Add the bias to each activation before multiplying by the weight\n",
    "        # Equivalent to computing w * (a + b) for each kernel position\n",
    "        modified_input = F.unfold(x, kernel_size=self.weight.shape[2:], stride=self.stride, padding=self.padding)\n",
    "        print(modified_input)\n",
    "        print(modified_input.shape)\n",
    "        modified_input = modified_input.view(\n",
    "            x.shape[0], x.shape[1], self.weight.shape[2], self.weight.shape[3], -1\n",
    "        )\n",
    "        print(modified_input)\n",
    "        print(modified_input.shape)\n",
    "        modified_input = modified_input + self.bias.unsqueeze(-1)\n",
    "        print(modified_input)\n",
    "        print(modified_input.shape)\n",
    "        modified_input = modified_input.view(x.shape[0], -1, modified_input.shape[-1])\n",
    "        print(modified_input)\n",
    "        print(modified_input.shape)\n",
    "        \n",
    "        print(self.weight)\n",
    "        print(self.weight.shape)\n",
    "        # Perform the convolution with the modified input\n",
    "        weight_reshaped = self.weight.view(self.weight.shape[0], -1)\n",
    "        print(weight_reshaped)\n",
    "        print(weight_reshaped.shape)\n",
    "        print(weight_reshaped.shape[0])\n",
    "        print(weight_reshaped.shape[1])\n",
    "        print(weight_reshaped.shape[1])\n",
    "        print(weight_reshaped.reshape(1, weight_reshaped.shape[0]*weight_reshaped.shape[1]).unsqueeze(-1).repeat(1, 1, 2))\n",
    "        print(weight_reshaped.reshape(1, weight_reshaped.shape[0]*weight_reshaped.shape[1]).unsqueeze(-1).repeat(1, 1, 2).shape)\n",
    "        print(weight_reshaped.unsqueeze(0).expand(x.shape[0], -1, -1))\n",
    "        print(weight_reshaped.unsqueeze(0).expand(x.shape[0], -1, -1).shape)\n",
    "        output = torch.bmm(weight_reshaped.unsqueeze(0).expand(x.shape[0], -1, -1), modified_input)\n",
    "        output = output.view(x.shape[0], self.weight.shape[0], int(x.shape[2]/self.stride), int(x.shape[3]/self.stride))\n",
    "\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "bika_conv = BiKAConv2D(in_channels=1, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "input_tensor  = torch.arange(0, 16, dtype=torch.float32).reshape(1,1,4,4)\n",
    "output_tensor = bika_conv(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a278449-8168-44ac-a20d-ded07c23b21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.,  1.,  2.,  3.],\n",
      "          [ 4.,  5.,  6.,  7.],\n",
      "          [ 8.,  9., 10., 11.],\n",
      "          [12., 13., 14., 15.]]]])\n",
      "torch.Size([1, 1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor)\n",
    "print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc688e0-b2e8-4f15-8ffa-3e826e902de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_tensor)\n",
    "print(output_tensor.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
