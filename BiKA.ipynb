{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9716d63-eedf-4466-abe4-19ba8a633fb5",
   "metadata": {},
   "source": [
    "# Define BiKA Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f42cc2af-891e-488f-8a2f-9c4808a378d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a09ecd-5136-469a-952a-0e86ae3f8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output during inference: tensor([ 1., -1., -1.,  1.], grad_fn=<CustomSignFunctionBackward>)\n",
      "Gradient during training: tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "class CustomSignFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Save the input for backward computation\n",
    "        ctx.save_for_backward(input)\n",
    "        # Output +1 for input > 0, else -1 (including for input == 0)\n",
    "        return torch.where(input > 0, torch.tensor(1.0, device=input.device), torch.tensor(-1.0, device=input.device))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve the input saved in the forward pass\n",
    "        input, = ctx.saved_tensors\n",
    "        # Gradient of the input is the same as the gradient output (STE)\n",
    "        grad_input = grad_output.clone()\n",
    "        # Pass the gradient only where input was non-zero, otherwise set it to 0\n",
    "        grad_input[input.abs() > 0] = grad_output[input.abs() > 0]\n",
    "        return grad_input\n",
    "\n",
    "# Wrapper class for convenience\n",
    "class CustomSignActivation(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomSignActivation, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return CustomSignFunction.apply(input)\n",
    "\n",
    "# Example usage:\n",
    "sign_activation = CustomSignActivation()\n",
    "\n",
    "# Test the forward pass\n",
    "x = torch.tensor([2.0, -3.0, 0.0, 1.5], requires_grad=True)\n",
    "output = sign_activation(x)\n",
    "print(\"Output during inference:\", output)\n",
    "\n",
    "# Test the backward pass (gradient computation during training)\n",
    "loss = output.sum()  # Just an example loss\n",
    "loss.backward()\n",
    "print(\"Gradient during training:\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35ab553d-22ba-4ce6-a69e-d131fb55c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiKALinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(BiKALinear, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.sign = CustomSignActivation()\n",
    "            \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Expand the input to match the bias shape for broadcasting\n",
    "        # x is of shape (batch_size, in_features)\n",
    "        # Expand bias matrix to (batch_size, out_features, in_features)\n",
    "        x = x.unsqueeze(1) + self.bias.unsqueeze(0)\n",
    "        \n",
    "        # Perform element-wise multiplication with weights\n",
    "        x = x * self.weight.unsqueeze(0)\n",
    "        \n",
    "        # Apply sign function: -1 for negative and 0, 1 for positive\n",
    "        x = self.sign(x)\n",
    "        \n",
    "        # Sum the thresholded products along the input features dimension\n",
    "        x = torch.sum(x, dim=-1) \n",
    "\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "bika_linear = BiKALinear(in_features=2, out_features=3)\n",
    "input_tensor  = torch.randn(3, 2)  # Batch of 3, 10 input features each\n",
    "output_tensor = bika_linear(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9c6ec69-929f-4e22-8c17-689ae35784ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiKAOut(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(BiKAOut, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.sign = CustomSignActivation()\n",
    "            \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Expand the input to match the bias shape for broadcasting\n",
    "        # x is of shape (batch_size, in_features)\n",
    "        # Expand bias matrix to (batch_size, out_features, in_features)\n",
    "        x = x.unsqueeze(1) + self.bias.unsqueeze(0)\n",
    "        \n",
    "        # Perform element-wise multiplication with weights\n",
    "        x = x * self.weight.unsqueeze(0)\n",
    "        \n",
    "        # Sum the thresholded products along the input features dimension\n",
    "        x = torch.sum(x, dim=-1) \n",
    "\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "bika_out = BiKAOut(in_features=2, out_features=3)\n",
    "input_tensor  = torch.randn(3, 2)  # Batch of 3, 10 input features each\n",
    "output_tensor = bika_out(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "427c2f6c-096f-4b6a-9b45-3a4379a0aa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1654,  0.6404],\n",
      "        [-1.2658, -0.0970],\n",
      "        [ 1.0283,  0.2005]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor)\n",
    "print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38906c59-f428-4c2e-b4b6-65362ec34888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.6167, -0.5201],\n",
      "        [-0.0363, -0.2454],\n",
      "        [ 0.0849,  0.3270]], requires_grad=True)\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(bika_linear.weight)\n",
    "print(bika_linear.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "325140eb-8269-413f-af18-5a5845eaedf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4240,  0.6646],\n",
      "        [ 0.6004, -0.5318],\n",
      "        [-0.6229,  0.3709]], requires_grad=True)\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(bika_linear.bias)\n",
    "print(bika_linear.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "325f3c88-7bad-4589-b8b9-464f8222af25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5490,  0.1593,  0.5842],\n",
      "        [ 1.1128,  0.0777,  0.3459],\n",
      "        [-0.1891, -0.4607, -0.8392]], grad_fn=<SumBackward1>)\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(output_tensor)\n",
    "print(output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aaa43d-21ca-457a-affd-82b2d289f818",
   "metadata": {},
   "source": [
    "# Try BiKA MLP with MNIST and output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12f1b71b-6b55-44a6-b75a-83cc13602bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e32827cb-5c2d-4750-82b3-75c0e160c3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = torchvision.datasets.MNIST('./data/', \n",
    "                                        train=True, download=True,\n",
    "                                        transform=torchvision.transforms.Compose\n",
    "                                        ([\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            #torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                            torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "                                        ]))\n",
    "data_test = torchvision.datasets.MNIST('./data/', \n",
    "                                       train=False, download=True,\n",
    "                                       transform=torchvision.transforms.Compose\n",
    "                                       ([\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            #torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                            torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "                                       ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "308351c5-ce72-46d9-ad4e-584dfe360b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size_train = 512\n",
    "batch_size_test = 1000\n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(dataset=data_train,\n",
    "                                                batch_size=batch_size_train, \n",
    "                                                shuffle=True)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset=data_test,\n",
    "                                               batch_size=batch_size_test, \n",
    "                                               shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "944df90d-dc16-44f2-b3aa-b3c617c00e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8980,  0.1373,\n",
      "          0.9922,  0.8118, -0.1765, -0.9765, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9059,  0.5922,  0.9843,\n",
      "          0.9843,  0.9843,  0.9373, -0.5765, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -0.8745,  0.5529,  0.9922,  0.9843,\n",
      "          0.9843,  0.9294,  0.2941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -0.8510,  0.5294,  0.9843,  0.9922,  0.9843,\n",
      "          0.7490, -0.1529, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -0.2863,  0.6392,  0.9843,  0.9843,  0.9922,  0.6235,\n",
      "         -0.7804, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -0.5608,  0.7255,  0.9843,  0.9843,  0.9843,  0.9216, -0.7961,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000,  0.5216,  0.9843,  0.9843,  0.9843,  0.3490, -0.1843, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -0.2235,  0.9216,  0.9843,  0.9843,  0.9843, -0.5451, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6471,\n",
      "          0.9373,  0.9843,  0.9843,  0.9843,  0.9843,  0.8980,  0.8118,  0.7176,\n",
      "         -0.1373, -0.7098, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6157,\n",
      "          0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.9922,  0.9843,\n",
      "          0.9843,  0.9294,  0.2157, -0.7725, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8431,  0.6784,\n",
      "          0.9922,  0.9922,  0.9922,  0.9137,  0.8980,  0.8980,  0.4510,  0.9137,\n",
      "          0.9922,  1.0000,  0.9922,  0.2706, -0.7569, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.1608,  0.9843,\n",
      "          0.9843,  0.9843,  0.0588, -0.8824, -1.0000, -1.0000, -1.0000, -0.9137,\n",
      "          0.4353,  0.9216,  0.9843,  0.9843,  0.3176, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.2314,  0.9843,\n",
      "          0.9843,  0.9843, -0.6235, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -0.2549,  0.9843,  0.9843,  0.7333, -0.8275, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3961,  0.9843,\n",
      "          0.9843,  0.9843, -0.6235, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -0.6863,  0.8824,  0.9843,  0.9843, -0.5373, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9137,  0.5137,\n",
      "          0.9843,  0.9843,  0.0510, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -0.9059,  0.5608,  0.9843,  0.9843,  0.2235, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3882,\n",
      "          0.9843,  0.9843,  0.8275,  0.1529, -0.9451, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -0.6157,  0.9843,  0.9843,  0.7961, -0.5765, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9216,\n",
      "         -0.2392,  0.9843,  0.9843,  0.9843,  0.2392, -0.6471, -1.0000, -0.9294,\n",
      "         -0.3255,  0.3725,  0.9843,  0.9843,  0.3176, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -0.9294, -0.2314,  0.9843,  0.9843,  0.9843,  0.8510,  0.7176,  0.7333,\n",
      "          0.9843,  0.9843,  0.9843,  0.9843,  0.3176, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -0.9451, -0.4980,  0.1294,  0.1294,  0.5373,  0.9922,  0.9843,\n",
      "          0.9843,  0.9843,  0.9843,  0.7412, -0.6941, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4980,  0.4980,  0.5843,\n",
      "          0.3961,  0.4902, -0.1843, -0.9373, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = enumerate(data_loader_train)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data[0][0])\n",
    "example_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b2a95ff-ea7a-4e26-9c24-854d71c77eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fffae836-5a45-4667-a622-b71b7b79eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1*28*28      \n",
    "hidden1 = 64      \n",
    "hidden2 = 64\n",
    "hidden3 = 64\n",
    "num_classes = 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e99d4da-52ea-4f61-88a3-6d1915f948c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class BiKA_MNIST(Module):\n",
    "    def __init__(self):\n",
    "        super(BiKA_MNIST, self).__init__()\n",
    "        \n",
    "        self.fc0   = BiKALinear(in_features=input_size, out_features=hidden1)\n",
    "        \n",
    "        self.fc1   = BiKALinear(in_features=hidden1, out_features=hidden2)\n",
    "        \n",
    "        self.fc2   = BiKALinear(in_features=hidden2, out_features=hidden3)\n",
    "        \n",
    "        self.out   = BiKAOut(in_features=hidden3, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = x.reshape(x.shape[0], -1)\n",
    "        \n",
    "        out = self.fc0(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.out(out)\n",
    "        \n",
    "        return out\n",
    "   \n",
    "model = BiKA_MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21b349be-f48a-4668-a3e4-0330a4f9b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    losses = []\n",
    "    # ensure model is in training mode\n",
    "    model.train()    \n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):        \n",
    "        inputs, target = data\n",
    "        #inputs, target = inputs.cuda(), target.cuda()\n",
    "        inputs, target = Variable(inputs), Variable(target)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _,pred = torch.max(outputs.data,1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs,target)\n",
    " \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # keep track of loss value\n",
    "        losses.append(loss.data.numpy()) \n",
    "           \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98efeebc-78b2-484a-92a8-d54ab7375d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def test(model, test_loader):    \n",
    "    # ensure model is in eval mode\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, target = data\n",
    "            #inputs, target = inputs.cuda(), target.cuda()\n",
    "            inputs, target = Variable(inputs),Variable(target)\n",
    "            output = model(inputs)\n",
    "            #output = torch.sigmoid(output_orig)  \n",
    "            _,pred = torch.max(output,1)\n",
    "            # compare against a threshold of 0.5 to generate 0/1\n",
    "            y_true.extend(target.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beeb67fa-cc4e-40c2-be6e-47391d8b0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "learn_rate = 0.001 \n",
    "\n",
    "def display_loss_plot(losses, title=\"Training loss\", xlabel=\"Iterations\", ylabel=\"Loss\"):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fa929fb-5420-4473-98b5-4feaed3e09e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss criterion and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b09afe0-b490-435e-823e-f1e1a7999dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 0.463429 test accuracy = 0.850200:  35%|â–Ž| 7/20 [03:42<06:52, 31\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_384158/1117317652.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training loss = %f test accuracy = %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_384158/3486183246.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Software/Conda_Envs/Brevitas_old/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Software/Conda_Envs/Brevitas_old/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;32m~/Projects/Software/Conda_Envs/Brevitas_old/lib/python3.7/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    251\u001b[0m                                \"of them.\")\n\u001b[1;32m    252\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_384158/3665646097.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mgrad_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Pass the gradient only where input was non-zero, otherwise set it to 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mgrad_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "running_loss = []\n",
    "running_test_acc = []\n",
    "t = trange(num_epochs, desc=\"Training loss\", leave=True)\n",
    "\n",
    "for epoch in t:\n",
    "        loss_epoch = train(model, data_loader_train, optimizer, criterion)\n",
    "        test_acc = test(model, data_loader_test)\n",
    "        t.set_description(\"Training loss = %f test accuracy = %f\" % (np.mean(loss_epoch), test_acc))\n",
    "        t.refresh() # to show immediately the update           \n",
    "        running_loss.append(loss_epoch)\n",
    "        running_test_acc.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f7e524-799e-403d-a9b9-6a4b26fa1910",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_per_epoch = [np.mean(loss_per_epoch) for loss_per_epoch in running_loss]\n",
    "display_loss_plot(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e88e03b-ace4-4ae2-965a-8e85106ff5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_per_epoch = [np.mean(acc_per_epoch) for acc_per_epoch in running_test_acc]\n",
    "display_loss_plot(acc_per_epoch, title=\"Test accuracy\", ylabel=\"Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5648bc9-c478-47d1-ab86-912f11108936",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, data_loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ab4e4-be2e-47e6-a639-ac1a97c36e8d",
   "metadata": {},
   "source": [
    "# Try BiKA MLP with MNIST without output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1948a1c-9d08-4662-90e3-36751e3b37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1*28*28      \n",
    "hidden1 = 64      \n",
    "num_classes = 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6f945-fc45-43e4-a439-ae1fd9690c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class BiKA_MNIST(Module):\n",
    "    def __init__(self):\n",
    "        super(BiKA_MNIST, self).__init__()\n",
    "        \n",
    "        self.fc0   = BiKALinear(in_features=input_size, out_features=hidden1)\n",
    "        \n",
    "        self.fc1   = BiKALinear(in_features=hidden1, out_features=hidden2)\n",
    "        \n",
    "        self.fc2   = BiKALinear(in_features=hidden2, out_features=hidden3)\n",
    "        \n",
    "        self.out   = BiKALinear(in_features=hidden3, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = x.reshape(x.shape[0], -1)\n",
    "        \n",
    "        out = self.fc0(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.out(out)\n",
    "        \n",
    "        return out\n",
    "   \n",
    "model = BiKA_MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9329f40-4e98-410b-a178-8c44ee0e90e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "running_loss = []\n",
    "running_test_acc = []\n",
    "t = trange(num_epochs, desc=\"Training loss\", leave=True)\n",
    "\n",
    "for epoch in t:\n",
    "        loss_epoch = train(model, data_loader_train, optimizer, criterion)\n",
    "        test_acc = test(model, data_loader_test)\n",
    "        t.set_description(\"Training loss = %f test accuracy = %f\" % (np.mean(loss_epoch), test_acc))\n",
    "        t.refresh() # to show immediately the update           \n",
    "        running_loss.append(loss_epoch)\n",
    "        running_test_acc.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ed7322-2fb5-415a-b96a-ad8032d3ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_per_epoch = [np.mean(loss_per_epoch) for loss_per_epoch in running_loss]\n",
    "display_loss_plot(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d77c7-1925-4e7a-b068-7b1105368e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_per_epoch = [np.mean(acc_per_epoch) for acc_per_epoch in running_test_acc]\n",
    "display_loss_plot(acc_per_epoch, title=\"Test accuracy\", ylabel=\"Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8768810-8694-4155-b439-32fef9fd2e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, data_loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684946b6-b4db-4bc7-ad23-036d19598391",
   "metadata": {},
   "source": [
    "# Try larger BiKA MLP with MNIST without output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2910412-3321-4afa-bb29-4cf2005e3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1*28*28      \n",
    "hidden1 = 1024      \n",
    "num_classes = 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f8a63-bb5b-444f-acbb-2db046ab737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class BiKA_MNIST(Module):\n",
    "    def __init__(self):\n",
    "        super(BiKA_MNIST, self).__init__()\n",
    "        \n",
    "        self.fc0   = BiKALinear(in_features=input_size, out_features=hidden1)\n",
    "        \n",
    "        self.fc1   = BiKALinear(in_features=hidden1, out_features=hidden2)\n",
    "        \n",
    "        self.fc2   = BiKALinear(in_features=hidden2, out_features=hidden3)\n",
    "        \n",
    "        self.out   = BiKALinear(in_features=hidden3, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = x.reshape(x.shape[0], -1)\n",
    "        \n",
    "        out = self.fc0(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.out(out)\n",
    "        \n",
    "        return out\n",
    "   \n",
    "model = BiKA_MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c95a97a-b1f8-4e7c-9a33-d169c7871817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "running_loss = []\n",
    "running_test_acc = []\n",
    "t = trange(num_epochs, desc=\"Training loss\", leave=True)\n",
    "\n",
    "for epoch in t:\n",
    "        loss_epoch = train(model, data_loader_train, optimizer, criterion)\n",
    "        test_acc = test(model, data_loader_test)\n",
    "        t.set_description(\"Training loss = %f test accuracy = %f\" % (np.mean(loss_epoch), test_acc))\n",
    "        t.refresh() # to show immediately the update           \n",
    "        running_loss.append(loss_epoch)\n",
    "        running_test_acc.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3c8e4d-1466-4437-919f-1cd9623ace31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_per_epoch = [np.mean(loss_per_epoch) for loss_per_epoch in running_loss]\n",
    "display_loss_plot(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be26a1c7-8540-4c55-9ec3-126f7f628a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_per_epoch = [np.mean(acc_per_epoch) for acc_per_epoch in running_test_acc]\n",
    "display_loss_plot(acc_per_epoch, title=\"Test accuracy\", ylabel=\"Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103a9e6-dc8e-449e-b8f2-7f4f460076e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, data_loader_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
